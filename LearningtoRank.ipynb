{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LearningtoRank.ipynb","provenance":[],"mount_file_id":"1d-ZmYD_gihSNC6LKI8rJ-DGVTwYgURkm","authorship_tag":"ABX9TyNiPPAeB5ZOaJkoY4syZWh4"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fqqMhqB-n3OH","executionInfo":{"status":"ok","timestamp":1606295993676,"user_tz":-330,"elapsed":9509,"user":{"displayName":"Nitisha Bharathi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxfqpJCwaLhBHRR5fCU7jXMq8oqDEIzm48OmPLmM=s64","userId":"17071592395864823246"}},"outputId":"b3d3cc9b-2e5d-4341-80f0-02b53cb12c2f"},"source":["!pip install LambdaRankNN\n","!pip install pyltr"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting LambdaRankNN\n","  Downloading https://files.pythonhosted.org/packages/2d/96/b10f198c7d652c50c4e1458a8517e49885aa79e1d288e96e1d0a9c74092b/LambdaRankNN-0.1.1-py3-none-any.whl\n","Installing collected packages: LambdaRankNN\n","Successfully installed LambdaRankNN-0.1.1\n","Collecting pyltr\n","  Downloading https://files.pythonhosted.org/packages/29/01/e7120dffc8bb40307002a51b85810ef714ee3faed260c416e9ae38feb282/pyltr-0.2.6-py3-none-any.whl\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyltr) (1.4.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pyltr) (1.15.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pyltr) (0.22.2.post1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pyltr) (1.1.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyltr) (1.18.5)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pyltr) (0.17.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->pyltr) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->pyltr) (2.8.1)\n","Installing collected packages: pyltr\n","Successfully installed pyltr-0.2.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xuoiJDiFXVeJ","executionInfo":{"status":"ok","timestamp":1606295997944,"user_tz":-330,"elapsed":13224,"user":{"displayName":"Nitisha Bharathi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxfqpJCwaLhBHRR5fCU7jXMq8oqDEIzm48OmPLmM=s64","userId":"17071592395864823246"}},"outputId":"58c7e88a-b6e1-4097-dcde-924839bd4c79"},"source":["from google.colab import drive,files\n","from google.colab import drive,files\n","drive.mount('/content/drive')\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import os\n","import numpy as np\n","import pandas as pd\n","from collections import Counter\n","from sklearn.model_selection import GridSearchCV\n","\n","\n","#Lambdaranknn library for lambdarank and ranknet in python\n","from LambdaRankNN import RankNetNN, LambdaRankNN\n","#Pyltr library for lambdamart in python\n","import pyltr\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim.lr_scheduler import CyclicLR"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CQSel5VvX-Au","executionInfo":{"status":"ok","timestamp":1606296010895,"user_tz":-330,"elapsed":3116,"user":{"displayName":"Nitisha Bharathi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxfqpJCwaLhBHRR5fCU7jXMq8oqDEIzm48OmPLmM=s64","userId":"17071592395864823246"}}},"source":["def load_data(filename):\n","    with open(filename, 'r') as f:\n","        lines = f.readlines()\n","        x, y, q = [], [], []\n","        info = []\n","        for line in lines:\n","            feature, comment = line.split(' #')\n","            feature = feature.split()\n","            yi = feature[0]\n","            qi = feature[1].split(':')[-1]\n","            xi = feature[2:]\n","            xi = [f.split(':')[-1] for f in xi]\n","            x.append(xi)\n","            y.append(yi)\n","            q.append(qi)\n","            info.append(comment)\n","        return np.array(x, dtype=np.float32), np.array(y, dtype=np.int), np.array(q, dtype=np.int)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yrHGALNxYfFC"},"source":["# rankNet\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7THeoFBFYsEb","executionInfo":{"status":"ok","timestamp":1606241307057,"user_tz":-330,"elapsed":123375,"user":{"displayName":"Nitisha Bharathi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxfqpJCwaLhBHRR5fCU7jXMq8oqDEIzm48OmPLmM=s64","userId":"17071592395864823246"}},"outputId":"2c8d4798-7bc6-46a6-9416-4985f35b4e65"},"source":["# Reading dataset\n","train_X, train_y, train_queries = load_data(\"/content/drive/MyDrive/IR Package/train.txt\")\n","test_X, test_y, test_queries = load_data(\"/content/drive/MyDrive/IR Package/test.txt\")\n","print(\"Data Loaded\")\n","print(train_X.shape, train_y.shape)\n","\n","# Training RankNetNN model\n","ranker = RankNetNN(input_size=train_X.shape[1], \n","                   hidden_layer_sizes=(16,8,), \n","                   activation=('relu', 'relu',), \n","                   solver='adam')\n","epochs = 200\n","ndcg_at = 10\n","ranker.fit(train_X, train_y, train_queries, epochs= epochs)\n","train_y_pred = ranker.predict(train_X)\n","ranker.evaluate(train_X, train_y, train_queries, eval_at=ndcg_at)\n","test_y_pred = ranker.predict(test_X)\n","ranker.evaluate(test_X, test_y, test_queries, eval_at=ndcg_at)\n","\n","\n","df1 = pd.DataFrame(train_y_pred)\n","df3 = pd.DataFrame(test_y_pred)\n","\n","ranknet = pd.concat([df1, df3], axis = 1)\n","ranknet.columns = ['Train_pred', 'Test_pred']\n","ranknet.to_csv(\"ranknet.csv\", index = False)\n","#files.download('ranknet.csv')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Data Loaded\n","(42158, 46) (42158,)\n","Epoch 1/10\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.5515\n","Epoch 2/10\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.5294\n","Epoch 3/10\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.5207\n","Epoch 4/10\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.5146\n","Epoch 5/10\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.5099\n","Epoch 6/10\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.5056\n","Epoch 7/10\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.5024\n","Epoch 8/10\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.5003\n","Epoch 9/10\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.4982\n","Epoch 10/10\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.4961\n","ndcg: 0.6115024913740011\n","ndcg@10: 0.46909315006207725\n","ndcg@10: 0.4748094691564848\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eEjBP8I-_XCg"},"source":["# LambdaMart\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zfUa1A-bZmAf","executionInfo":{"status":"ok","timestamp":1606241757358,"user_tz":-330,"elapsed":573666,"user":{"displayName":"Nitisha Bharathi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxfqpJCwaLhBHRR5fCU7jXMq8oqDEIzm48OmPLmM=s64","userId":"17071592395864823246"}},"outputId":"7cdeaa7f-98f5-48a9-88ad-318159fe034b"},"source":["\n","with open( \"/content/drive/MyDrive/IR Package/train.txt\") as trainfile, \\\n","    open(\"/content/drive/MyDrive/IR Package/test.txt\") as valifile, \\\n","    open(\"/content/drive/MyDrive/IR Package/test.txt\") as evalfile:\n","  TX, Ty, Tqids, _ = pyltr.data.letor.read_dataset(trainfile)\n","  VX, Vy, Vqids, _ = pyltr.data.letor.read_dataset(valifile)\n","  EX, Ey, Eqids, _ = pyltr.data.letor.read_dataset(evalfile)\n","\n","metric = pyltr.metrics.NDCG(k=10)\n","monitor = pyltr.models.monitors.ValidationMonitor(\n","    VX, Vy, Vqids, metric=metric, stop_after=100)\n","\"\"\"\n","mart = pyltr.models.LambdaMART(\n","    metric=metric, \n","    query_subsample=0.5,\n","    max_leaf_nodes=10,\n","    min_samples_leaf=64,\n","    verbose=1)\n","params = {'n_estimators':[500, 1000, 2000, 5000], 'learning_rate':[0.01, 0.02, 0.1, 0.2, 0.5], 'subsample':[0.5,1], 'max_features':[0.5,'auto']}\n","grid = GridSearchCV(mart, params)\n","grid.fit(TX, Ty, Tqids, monitor=monitor)\n","print(\"Grid search results\")\n","print(grid.cv_results_.keys())\n","\"\"\"\n","ranker2 = pyltr.models.LambdaMART(\n","    metric=metric,\n","    n_estimators=1000,\n","    learning_rate=0.01,\n","    max_features=0.5,\n","    query_subsample=0.5,\n","    max_leaf_nodes=10,\n","    min_samples_leaf=16,\n","    verbose=1,\n",")\n","\n","ranker2.fit(TX, Ty, Tqids, monitor=monitor)\n","train_y_pred = ranker2.predict(TX)\n","print('Train NDCG@10:', metric.calc_mean(Tqids, Ty, train_y_pred))\n","\n","test_y_pred = ranker2.predict(EX)\n","print('Test NDCG@10:', metric.calc_mean(Eqids, Ey, test_y_pred))\n","\n","\n","df1 = pd.DataFrame(train_y_pred)\n","df3 = pd.DataFrame(test_y_pred)\n","mart = pd.concat([df1, df3], axis = 1)\n","mart.columns = ['Train_pred',  'Test_pred']\n","mart.to_csv(\"lambda_mart.csv\", index = False)\n","#files.download('lambda_mart.csv')"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" Iter  Train score  OOB Improve    Remaining                           Monitor Output \n","    1       0.2585       0.2417       15.46m      C:      0.2700 B:      0.2700 S:  0\n","    2       0.3491       0.0848       15.29m      C:      0.3785 B:      0.3785 S:  0\n","    3       0.3788       0.0184       15.47m      C:      0.3960 B:      0.3960 S:  0\n","    4       0.3606       0.0097       15.49m      C:      0.4026 B:      0.4026 S:  0\n","    5       0.3815       0.0048       15.48m      C:      0.4134 B:      0.4134 S:  0\n","    6       0.3984       0.0015       15.50m      C:      0.4166 B:      0.4166 S:  0\n","    7       0.3962       0.0090       15.42m      C:      0.4274 B:      0.4274 S:  0\n","    8       0.3829       0.0033       15.37m      C:      0.4297 B:      0.4297 S:  0\n","    9       0.3921       0.0023       15.40m      C:      0.4312 B:      0.4312 S:  0\n","   10       0.3923      -0.0003       15.41m      C:      0.4327 B:      0.4327 S:  0\n","   15       0.4218       0.0015       15.34m      C:      0.4446 B:      0.4446 S:  0\n","   20       0.4236      -0.0000       15.22m      C:      0.4487 B:      0.4487 S:  0\n","   25       0.4120      -0.0010       15.13m      C:      0.4493 B:      0.4506 S:  4\n","   30       0.4042       0.0007       15.02m      C:      0.4515 B:      0.4515 S:  0\n","   35       0.4356       0.0010       14.93m      C:      0.4507 B:      0.4518 S:  2\n","   40       0.4276       0.0006       14.86m      C:      0.4525 B:      0.4533 S:  4\n","   45       0.4284      -0.0008       14.78m      C:      0.4521 B:      0.4537 S:  3\n","   50       0.4431       0.0003       14.73m      C:      0.4515 B:      0.4537 S:  8\n","   60       0.4359       0.0002       14.63m      C:      0.4552 B:      0.4552 S:  0\n","   70       0.4308      -0.0001       14.49m      C:      0.4548 B:      0.4552 S: 10\n","   80       0.4307       0.0000       14.31m      C:      0.4566 B:      0.4569 S:  2\n","   90       0.4407      -0.0002       14.16m      C:      0.4579 B:      0.4579 S:  0\n","  100       0.4313      -0.0002       14.00m      C:      0.4600 B:      0.4617 S:  2\n","  120       0.4327       0.0001       13.69m      C:      0.4598 B:      0.4617 S: 22\n","  140       0.4307       0.0006       13.37m      C:      0.4613 B:      0.4617 S:  1\n","  160       0.4474       0.0001       13.40m      C:      0.4620 B:      0.4639 S:  3\n","  180       0.4498      -0.0004       13.13m      C:      0.4639 B:      0.4641 S:  1\n","  200       0.4501       0.0002       12.78m      C:      0.4636 B:      0.4649 S: 10\n","  220       0.4528      -0.0000       12.43m      C:      0.4651 B:      0.4651 S:  0\n","  240       0.4468      -0.0002       12.10m      C:      0.4667 B:      0.4671 S:  7\n","  260       0.4504       0.0002       11.76m      C:      0.4685 B:      0.4688 S:  2\n","  280       0.4441      -0.0001       11.42m      C:      0.4683 B:      0.4692 S: 14\n","  300       0.4368       0.0002       11.09m      C:      0.4691 B:      0.4693 S:  7\n","  320       0.4599      -0.0003       10.76m      C:      0.4698 B:      0.4709 S: 12\n","  340       0.4500      -0.0001       10.44m      C:      0.4714 B:      0.4714 S:  1\n","  360       0.4590      -0.0001       10.11m      C:      0.4720 B:      0.4725 S:  1\n","  380       0.4568      -0.0000        9.79m      C:      0.4721 B:      0.4734 S: 10\n","  400       0.4348       0.0002        9.48m      C:      0.4714 B:      0.4734 S: 30\n","  420       0.4542      -0.0001        9.16m      C:      0.4708 B:      0.4734 S: 50\n","  440       0.4524       0.0003        8.84m      C:      0.4706 B:      0.4734 S: 70\n","  460       0.4759       0.0006        8.52m      C:      0.4693 B:      0.4734 S: 90\n","Early termination at iteration  469\n","Train NDCG@10: 0.45528797040851837\n","Test NDCG@10: 0.47339805784804545\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-AJ6uOsUdgus"},"source":["# Lambda rank"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"dfp3uf7hdvG4","executionInfo":{"status":"ok","timestamp":1606243595826,"user_tz":-330,"elapsed":2412120,"user":{"displayName":"Nitisha Bharathi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxfqpJCwaLhBHRR5fCU7jXMq8oqDEIzm48OmPLmM=s64","userId":"17071592395864823246"}},"outputId":"0283f9ce-2ba6-44d8-c044-d275350ef7ab"},"source":["#Reading Dataset\n","train_X, train_y, train_queries = load_data(\"/content/drive/MyDrive/IR Package/train.txt\")\n","test_X, test_y, test_queries = load_data(\"/content/drive/MyDrive/IR Package/test.txt\")\n","print(\"Loaded data\")\n","\n","print(train_X.shape, train_y.shape)\n","\n","# Training LambdaRankNN model\n","ranker3 = LambdaRankNN(input_size=train_X.shape[1], \n","                      hidden_layer_sizes=(16,8,), \n","                      activation=('relu', 'relu',), \n","                      solver='adam')\n","epochs = 200\n","ndcg_at = 10\n","ranker3.fit(train_X, train_y, train_queries, epochs=epochs)\n","\n","train_y_pred = ranker3.predict(train_X)\n","print(\"Train evaluate\")\n","ranker3.evaluate(train_X, train_y, train_queries, eval_at=ndcg_at)\n","test_y_pred = ranker3.predict(test_X)\n","print(\"Test evaluate\")\n","ranker.evaluate(test_X, test_y, test_queries, eval_at=ndcg_at)\n","\n","df1 = pd.DataFrame(train_y_pred)\n","df3 = pd.DataFrame(test_y_pred)\n","\n","lambdarank = pd.concat([df1, df3], axis = 1)\n","lambdarank.columns = ['Train_pred', 'Test_pred']\n","lambdarank.to_csv(\"lambda_rank.csv\", index = False)\n","files.download('lambda_rank.csv')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loaded data\n","(42158, 46) (42158,)\n","Epoch 1/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0120\n","Epoch 2/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0114\n","Epoch 3/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0110\n","Epoch 4/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0107\n","Epoch 5/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0105\n","Epoch 6/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0104\n","Epoch 7/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0102\n","Epoch 8/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0101\n","Epoch 9/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0101\n","Epoch 10/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0100\n","Epoch 11/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0099\n","Epoch 12/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0099\n","Epoch 13/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0098\n","Epoch 14/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0098\n","Epoch 15/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0097\n","Epoch 16/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0097\n","Epoch 17/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0096\n","Epoch 18/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0096\n","Epoch 19/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0096\n","Epoch 20/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0095\n","Epoch 21/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0095\n","Epoch 22/200\n","7688/7688 [==============================] - 10s 1ms/step - loss: 0.0095\n","Epoch 23/200\n","7688/7688 [==============================] - 13s 2ms/step - loss: 0.0095\n","Epoch 24/200\n","7688/7688 [==============================] - 11s 1ms/step - loss: 0.0094\n","Epoch 25/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0094\n","Epoch 26/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0094\n","Epoch 27/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0094\n","Epoch 28/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0094\n","Epoch 29/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0094\n","Epoch 30/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0094\n","Epoch 31/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0094\n","Epoch 32/200\n","7688/7688 [==============================] - 13s 2ms/step - loss: 0.0093\n","Epoch 33/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0093\n","Epoch 34/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0093\n","Epoch 35/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0093\n","Epoch 36/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0093\n","Epoch 37/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0093\n","Epoch 38/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0093\n","Epoch 39/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0093\n","Epoch 40/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0093\n","Epoch 41/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0093\n","Epoch 42/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0093\n","Epoch 43/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0092\n","Epoch 44/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0092\n","Epoch 45/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0092\n","Epoch 46/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0092\n","Epoch 47/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0092\n","Epoch 48/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0092\n","Epoch 49/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0092\n","Epoch 50/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0092\n","Epoch 51/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0092\n","Epoch 52/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0092\n","Epoch 53/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0092\n","Epoch 54/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0092\n","Epoch 55/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0092\n","Epoch 56/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0092\n","Epoch 57/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0092\n","Epoch 58/200\n","7688/7688 [==============================] - 10s 1ms/step - loss: 0.0092\n","Epoch 59/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0092\n","Epoch 60/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0091\n","Epoch 61/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0091\n","Epoch 62/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0091\n","Epoch 63/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0091\n","Epoch 64/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0091\n","Epoch 65/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0091\n","Epoch 66/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0091\n","Epoch 67/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0091\n","Epoch 68/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0091\n","Epoch 69/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0091\n","Epoch 70/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0091\n","Epoch 71/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0091\n","Epoch 72/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0091\n","Epoch 73/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0091\n","Epoch 74/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0091\n","Epoch 75/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0091\n","Epoch 76/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0091\n","Epoch 77/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0090\n","Epoch 78/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0090\n","Epoch 79/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0090\n","Epoch 80/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0090\n","Epoch 81/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0090\n","Epoch 82/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0090\n","Epoch 83/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0090\n","Epoch 84/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0090\n","Epoch 85/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0090\n","Epoch 86/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0090\n","Epoch 87/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0090\n","Epoch 88/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0090\n","Epoch 89/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0090\n","Epoch 90/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0090\n","Epoch 91/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0090\n","Epoch 92/200\n","7688/7688 [==============================] - 10s 1ms/step - loss: 0.0090\n","Epoch 93/200\n","7688/7688 [==============================] - 10s 1ms/step - loss: 0.0090\n","Epoch 94/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 95/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 96/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 97/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 98/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 99/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 100/200\n","7688/7688 [==============================] - 13s 2ms/step - loss: 0.0089\n","Epoch 101/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 102/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 103/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 104/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 105/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 106/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 107/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 108/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 109/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 110/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 111/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 112/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 113/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 114/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 115/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 116/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 117/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 118/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 119/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 120/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 121/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0089\n","Epoch 122/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 123/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 124/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 125/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 126/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 127/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 128/200\n","7688/7688 [==============================] - 10s 1ms/step - loss: 0.0088\n","Epoch 129/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 130/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 131/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 132/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 133/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 134/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 135/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 136/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 137/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 138/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 139/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 140/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 141/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 142/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 143/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 144/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 145/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 146/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 147/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 148/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 149/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 150/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 151/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 152/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 153/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 154/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 155/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 156/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 157/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 158/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 159/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 160/200\n","7688/7688 [==============================] - 10s 1ms/step - loss: 0.0088\n","Epoch 161/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 162/200\n","7688/7688 [==============================] - 10s 1ms/step - loss: 0.0087\n","Epoch 163/200\n","7688/7688 [==============================] - 10s 1ms/step - loss: 0.0088\n","Epoch 164/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 165/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 166/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 167/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 168/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 169/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 170/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0088\n","Epoch 171/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 172/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 173/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 174/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 175/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 176/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 177/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 178/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 179/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 180/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 181/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 182/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 183/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 184/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 185/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 186/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 187/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 188/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 189/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 190/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 191/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 192/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 193/200\n","7688/7688 [==============================] - 10s 1ms/step - loss: 0.0087\n","Epoch 194/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 195/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 196/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 197/200\n","7688/7688 [==============================] - 10s 1ms/step - loss: 0.0087\n","Epoch 198/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 199/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","Epoch 200/200\n","7688/7688 [==============================] - 9s 1ms/step - loss: 0.0087\n","ndcg: 0.6247075020973414\n","Train evaluate\n","ndcg@10: 0.48658050228638744\n","Test evaluate\n","ndcg@10: 0.4748094691564848\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_8a08404b-e966-4ed1-87af-fd113f20eef9\", \"lambda_rank.csv\", 586083)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"rG3FbGsOfHkI"},"source":["\n","# Ensemble of Ranking Algorithms\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s95LSvgFfKU-","executionInfo":{"status":"ok","timestamp":1606243606799,"user_tz":-330,"elapsed":2423081,"user":{"displayName":"Nitisha Bharathi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxfqpJCwaLhBHRR5fCU7jXMq8oqDEIzm48OmPLmM=s64","userId":"17071592395864823246"}},"outputId":"f08c6c94-458f-4f61-eabb-c5530c92da76"},"source":["seed = 25\n","torch.manual_seed(seed)\n","\n","def read_data(filename):\n","    y1 = pd.read_csv(filename)['Test_pred']\n","    y1 = y1.dropna()\n","    y1 = torch.tensor(y1).unsqueeze(1)\n","    return y1\n","\n","print(\"Data Loading\")  \n","y1 = read_data(\"lambda_mart.csv\")\n","y2 = read_data(\"lambda_rank.csv\")\n","y3 = read_data(\"ranknet.csv\")\n","\n","print(y1.shape, y2.shape, y3.shape)\n","input_val = torch.cat([y1, y2, y3], dim = 1)\n","print(\"input_val\", input_val.shape)\n","\n","with open(\"/content/drive/MyDrive/IR Package/test.txt\") as evalfile:\n","  _, Ey, Eqids, _ = pyltr.data.letor.read_dataset(evalfile)\n","\n","print(\"Data Loading Complete\")\n","targets = torch.tensor(Ey).unsqueeze(1)\n","    \n","class FeedForwardNeuralNet(nn.Module):\n","    def __init__(self, inp, out, hdim1, hdim2):\n","        super().__init__()\n","        self.fc1 = nn.Linear(inp, hdim1)\n","        self.fc2 = nn.Linear(hdim1, hdim2)\n","        self.fc3 = nn.Linear(hdim2, out)\n","        self.relu = nn.ReLU(inplace = True)\n","        nn.init.xavier_uniform_(self.fc1.weight)\n","        nn.init.normal_(self.fc1.bias)\n","        nn.init.xavier_uniform_(self.fc2.weight)\n","        nn.init.normal_(self.fc2.bias)\n","        nn.init.xavier_uniform_(self.fc3.weight)\n","        nn.init.normal_(self.fc3.bias)\n","        \n","        \n","    def forward(self, input_val):\n","        x = self.fc1(input_val)\n","        x = self.relu(x)\n","        x = self.relu(self.fc2(x))\n","        out = self.fc3(x)\n","        return out\n","    \n","# Setting device type\n","if torch.cuda.is_available():\n","    print(\"Running on GPU\")\n","    device = torch.device(\"cuda:2\")\n","else:\n","    device = torch.device(\"cpu\")\n","    \n","print(\"Running on device type: {}\".format(device))\n","\n","num_epochs = 2000\n","batch_size = 30000\n","learning_rate = 1e-2\n","\n","\n","model = FeedForwardNeuralNet(3,1,8,8)\n","model.to(device)\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n","scheduler = CyclicLR(optimizer, base_lr=learning_rate, max_lr=1, mode=\"exp_range\", gamma=0.999)\n","\n","criterion = nn.MSELoss()\n","training_loss_list = []\n","\n","model.train()\n","for epoch in range(num_epochs):\n","    training_loss = 0\n","    for i in range(0,len(input_val), batch_size):\n","        start = i\n","        end = start + batch_size\n","        inputs = input_val[start:end,:].float()\n","        target = targets[start:end].float()\n","        inputs = inputs.to(device)\n","        target = target.to(device)\n","        out = model(inputs)\n","        optimizer.zero_grad()\n","        loss = torch.sqrt(criterion(out, target))\n","        training_loss += loss.item()\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), 1)\n","        optimizer.step()\n","    scheduler.step(training_loss)\n","    print(\"epoch\", epoch, \"training_loss_per_epoch\", training_loss)\n","    training_loss_list.append(training_loss)\n","\n","metric = pyltr.metrics.NDCG(k=10)\n","target1 = target.numpy().reshape(target.shape[0],)\n","out_r = out.detach().numpy().reshape(out.shape[0],)\n","print(\"NDCG Ensemble\", metric.calc_mean(Eqids, target1, out_r))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Data Loading\n","torch.Size([13652, 1]) torch.Size([13652, 1]) torch.Size([13652, 1])\n","input_val torch.Size([13652, 3])\n","Data Loading Complete\n","Running on device type: cpu\n","epoch 0 training_loss_per_epoch 1.672982096672058\n","epoch 1 training_loss_per_epoch 1.6266167163848877\n","epoch 2 training_loss_per_epoch 1.533496379852295\n","epoch 3 training_loss_per_epoch 1.4065755605697632\n","epoch 4 training_loss_per_epoch 1.256514549255371\n","epoch 5 training_loss_per_epoch 1.0961060523986816\n","epoch 6 training_loss_per_epoch 0.9418767094612122\n","epoch 7 training_loss_per_epoch 0.8267958164215088\n","epoch 8 training_loss_per_epoch 0.75444096326828\n","epoch 9 training_loss_per_epoch 0.7086610794067383\n","epoch 10 training_loss_per_epoch 0.6860628128051758\n","epoch 11 training_loss_per_epoch 0.6818895936012268\n","epoch 12 training_loss_per_epoch 0.6836795210838318\n","epoch 13 training_loss_per_epoch 0.6826696395874023\n","epoch 14 training_loss_per_epoch 0.6751812100410461\n","epoch 15 training_loss_per_epoch 0.6609942317008972\n","epoch 16 training_loss_per_epoch 0.6419809460639954\n","epoch 17 training_loss_per_epoch 0.6216437816619873\n","epoch 18 training_loss_per_epoch 0.6040931940078735\n","epoch 19 training_loss_per_epoch 0.5929285883903503\n","epoch 20 training_loss_per_epoch 0.589865505695343\n","epoch 21 training_loss_per_epoch 0.5934351086616516\n","epoch 22 training_loss_per_epoch 0.5996997952461243\n","epoch 23 training_loss_per_epoch 0.6042382717132568\n","epoch 24 training_loss_per_epoch 0.6044732928276062\n","epoch 25 training_loss_per_epoch 0.600504994392395\n","epoch 26 training_loss_per_epoch 0.5944032073020935\n","epoch 27 training_loss_per_epoch 0.5888610482215881\n","epoch 28 training_loss_per_epoch 0.5856632590293884\n","epoch 29 training_loss_per_epoch 0.5852099657058716\n","epoch 30 training_loss_per_epoch 0.5866957902908325\n","epoch 31 training_loss_per_epoch 0.588827908039093\n","epoch 32 training_loss_per_epoch 0.5904902219772339\n","epoch 33 training_loss_per_epoch 0.5910460352897644\n","epoch 34 training_loss_per_epoch 0.5903788805007935\n","epoch 35 training_loss_per_epoch 0.5888052582740784\n","epoch 36 training_loss_per_epoch 0.5868849754333496\n","epoch 37 training_loss_per_epoch 0.5852145552635193\n","epoch 38 training_loss_per_epoch 0.5842251777648926\n","epoch 39 training_loss_per_epoch 0.5840592980384827\n","epoch 40 training_loss_per_epoch 0.5845415592193604\n","epoch 41 training_loss_per_epoch 0.5852863192558289\n","epoch 42 training_loss_per_epoch 0.5858801007270813\n","epoch 43 training_loss_per_epoch 0.5860459804534912\n","epoch 44 training_loss_per_epoch 0.5857298970222473\n","epoch 45 training_loss_per_epoch 0.5850810408592224\n","epoch 46 training_loss_per_epoch 0.5843502879142761\n","epoch 47 training_loss_per_epoch 0.5837658643722534\n","epoch 48 training_loss_per_epoch 0.5834544897079468\n","epoch 49 training_loss_per_epoch 0.5834212899208069\n","epoch 50 training_loss_per_epoch 0.5835711359977722\n","epoch 51 training_loss_per_epoch 0.5837697982788086\n","epoch 52 training_loss_per_epoch 0.5838972330093384\n","epoch 53 training_loss_per_epoch 0.5838850736618042\n","epoch 54 training_loss_per_epoch 0.5837257504463196\n","epoch 55 training_loss_per_epoch 0.5834648609161377\n","epoch 56 training_loss_per_epoch 0.5831735134124756\n","epoch 57 training_loss_per_epoch 0.5829209089279175\n","epoch 58 training_loss_per_epoch 0.5827566385269165\n","epoch 59 training_loss_per_epoch 0.5826887488365173\n","epoch 60 training_loss_per_epoch 0.5826905369758606\n","epoch 61 training_loss_per_epoch 0.5827170610427856\n","epoch 62 training_loss_per_epoch 0.582724928855896\n","epoch 63 training_loss_per_epoch 0.58268803358078\n","epoch 64 training_loss_per_epoch 0.5826004147529602\n","epoch 65 training_loss_per_epoch 0.5824748873710632\n","epoch 66 training_loss_per_epoch 0.5823368430137634\n","epoch 67 training_loss_per_epoch 0.5822126865386963\n","epoch 68 training_loss_per_epoch 0.5821185111999512\n","epoch 69 training_loss_per_epoch 0.5820572972297668\n","epoch 70 training_loss_per_epoch 0.5820214152336121\n","epoch 71 training_loss_per_epoch 0.5819969773292542\n","epoch 72 training_loss_per_epoch 0.5819692015647888\n","epoch 73 training_loss_per_epoch 0.5819292664527893\n","epoch 74 training_loss_per_epoch 0.5818732976913452\n","epoch 75 training_loss_per_epoch 0.5818046927452087\n","epoch 76 training_loss_per_epoch 0.5817312002182007\n","epoch 77 training_loss_per_epoch 0.5816616415977478\n","epoch 78 training_loss_per_epoch 0.5816014409065247\n","epoch 79 training_loss_per_epoch 0.5815532803535461\n","epoch 80 training_loss_per_epoch 0.5815146565437317\n","epoch 81 training_loss_per_epoch 0.5814812779426575\n","epoch 82 training_loss_per_epoch 0.5814476013183594\n","epoch 83 training_loss_per_epoch 0.5814098119735718\n","epoch 84 training_loss_per_epoch 0.581366777420044\n","epoch 85 training_loss_per_epoch 0.5813204646110535\n","epoch 86 training_loss_per_epoch 0.5812728404998779\n","epoch 87 training_loss_per_epoch 0.5812260508537292\n","epoch 88 training_loss_per_epoch 0.5811830163002014\n","epoch 89 training_loss_per_epoch 0.5811445713043213\n","epoch 90 training_loss_per_epoch 0.5811101794242859\n","epoch 91 training_loss_per_epoch 0.5810781121253967\n","epoch 92 training_loss_per_epoch 0.581046998500824\n","epoch 93 training_loss_per_epoch 0.5810157656669617\n","epoch 94 training_loss_per_epoch 0.5809831619262695\n","epoch 95 training_loss_per_epoch 0.5809492468833923\n","epoch 96 training_loss_per_epoch 0.5809150338172913\n","epoch 97 training_loss_per_epoch 0.5808815956115723\n","epoch 98 training_loss_per_epoch 0.580849289894104\n","epoch 99 training_loss_per_epoch 0.5808187127113342\n","epoch 100 training_loss_per_epoch 0.5807898044586182\n","epoch 101 training_loss_per_epoch 0.5807620882987976\n","epoch 102 training_loss_per_epoch 0.5807347297668457\n","epoch 103 training_loss_per_epoch 0.5807071924209595\n","epoch 104 training_loss_per_epoch 0.5806794166564941\n","epoch 105 training_loss_per_epoch 0.5806513428688049\n","epoch 106 training_loss_per_epoch 0.580623209476471\n","epoch 107 training_loss_per_epoch 0.5805954337120056\n","epoch 108 training_loss_per_epoch 0.5805681347846985\n","epoch 109 training_loss_per_epoch 0.5805415511131287\n","epoch 110 training_loss_per_epoch 0.5805156230926514\n","epoch 111 training_loss_per_epoch 0.5804902911186218\n","epoch 112 training_loss_per_epoch 0.5804653763771057\n","epoch 113 training_loss_per_epoch 0.5804407596588135\n","epoch 114 training_loss_per_epoch 0.5804161429405212\n","epoch 115 training_loss_per_epoch 0.5803915858268738\n","epoch 116 training_loss_per_epoch 0.5803670883178711\n","epoch 117 training_loss_per_epoch 0.5803427696228027\n","epoch 118 training_loss_per_epoch 0.5803187489509583\n","epoch 119 training_loss_per_epoch 0.5802951455116272\n","epoch 120 training_loss_per_epoch 0.5802718997001648\n","epoch 121 training_loss_per_epoch 0.5802488923072815\n","epoch 122 training_loss_per_epoch 0.5802261233329773\n","epoch 123 training_loss_per_epoch 0.580203652381897\n","epoch 124 training_loss_per_epoch 0.5801811814308167\n","epoch 125 training_loss_per_epoch 0.5801588892936707\n","epoch 126 training_loss_per_epoch 0.5801367163658142\n","epoch 127 training_loss_per_epoch 0.5801147818565369\n","epoch 128 training_loss_per_epoch 0.5800929665565491\n","epoch 129 training_loss_per_epoch 0.5800714492797852\n","epoch 130 training_loss_per_epoch 0.5800502300262451\n","epoch 131 training_loss_per_epoch 0.5800292491912842\n","epoch 132 training_loss_per_epoch 0.5800085067749023\n","epoch 133 training_loss_per_epoch 0.5799878835678101\n","epoch 134 training_loss_per_epoch 0.5799674987792969\n","epoch 135 training_loss_per_epoch 0.5799473524093628\n","epoch 136 training_loss_per_epoch 0.5799272656440735\n","epoch 137 training_loss_per_epoch 0.5799073576927185\n","epoch 138 training_loss_per_epoch 0.5798876285552979\n","epoch 139 training_loss_per_epoch 0.5798680782318115\n","epoch 140 training_loss_per_epoch 0.5798487663269043\n","epoch 141 training_loss_per_epoch 0.5798296928405762\n","epoch 142 training_loss_per_epoch 0.5798109173774719\n","epoch 143 training_loss_per_epoch 0.5797922611236572\n","epoch 144 training_loss_per_epoch 0.5797737240791321\n","epoch 145 training_loss_per_epoch 0.579755425453186\n","epoch 146 training_loss_per_epoch 0.5797372460365295\n","epoch 147 training_loss_per_epoch 0.5797191858291626\n","epoch 148 training_loss_per_epoch 0.5797012448310852\n","epoch 149 training_loss_per_epoch 0.5796835422515869\n","epoch 150 training_loss_per_epoch 0.5796658992767334\n","epoch 151 training_loss_per_epoch 0.5796484351158142\n","epoch 152 training_loss_per_epoch 0.5796310901641846\n","epoch 153 training_loss_per_epoch 0.5796139240264893\n","epoch 154 training_loss_per_epoch 0.5795968770980835\n","epoch 155 training_loss_per_epoch 0.5795800089836121\n","epoch 156 training_loss_per_epoch 0.5795632004737854\n","epoch 157 training_loss_per_epoch 0.5795465707778931\n","epoch 158 training_loss_per_epoch 0.5795300602912903\n","epoch 159 training_loss_per_epoch 0.5795136094093323\n","epoch 160 training_loss_per_epoch 0.5794973969459534\n","epoch 161 training_loss_per_epoch 0.5794811844825745\n","epoch 162 training_loss_per_epoch 0.5794651508331299\n","epoch 163 training_loss_per_epoch 0.5794491171836853\n","epoch 164 training_loss_per_epoch 0.5794332027435303\n","epoch 165 training_loss_per_epoch 0.5794174075126648\n","epoch 166 training_loss_per_epoch 0.5794017314910889\n","epoch 167 training_loss_per_epoch 0.5793861150741577\n","epoch 168 training_loss_per_epoch 0.5793706774711609\n","epoch 169 training_loss_per_epoch 0.5793552398681641\n","epoch 170 training_loss_per_epoch 0.5793398022651672\n","epoch 171 training_loss_per_epoch 0.5793246626853943\n","epoch 172 training_loss_per_epoch 0.5793095231056213\n","epoch 173 training_loss_per_epoch 0.5792945623397827\n","epoch 174 training_loss_per_epoch 0.5792797803878784\n","epoch 175 training_loss_per_epoch 0.5792651176452637\n","epoch 176 training_loss_per_epoch 0.5792506337165833\n","epoch 177 training_loss_per_epoch 0.5792362689971924\n","epoch 178 training_loss_per_epoch 0.5792220234870911\n","epoch 179 training_loss_per_epoch 0.5792078375816345\n","epoch 180 training_loss_per_epoch 0.5791937112808228\n","epoch 181 training_loss_per_epoch 0.5791797637939453\n","epoch 182 training_loss_per_epoch 0.5791659355163574\n","epoch 183 training_loss_per_epoch 0.5791521668434143\n","epoch 184 training_loss_per_epoch 0.5791383981704712\n","epoch 185 training_loss_per_epoch 0.5791245698928833\n","epoch 186 training_loss_per_epoch 0.5791109204292297\n","epoch 187 training_loss_per_epoch 0.5790972709655762\n","epoch 188 training_loss_per_epoch 0.5790838003158569\n","epoch 189 training_loss_per_epoch 0.5790703296661377\n","epoch 190 training_loss_per_epoch 0.5790569186210632\n","epoch 191 training_loss_per_epoch 0.5790436863899231\n","epoch 192 training_loss_per_epoch 0.5790305137634277\n","epoch 193 training_loss_per_epoch 0.5790174007415771\n","epoch 194 training_loss_per_epoch 0.5790042877197266\n","epoch 195 training_loss_per_epoch 0.5789913535118103\n","epoch 196 training_loss_per_epoch 0.5789785981178284\n","epoch 197 training_loss_per_epoch 0.5789657831192017\n","epoch 198 training_loss_per_epoch 0.5789531469345093\n","epoch 199 training_loss_per_epoch 0.5789405703544617\n","epoch 200 training_loss_per_epoch 0.5789280533790588\n","epoch 201 training_loss_per_epoch 0.5789156556129456\n","epoch 202 training_loss_per_epoch 0.578903317451477\n","epoch 203 training_loss_per_epoch 0.5788910388946533\n","epoch 204 training_loss_per_epoch 0.5788788199424744\n","epoch 205 training_loss_per_epoch 0.5788666605949402\n","epoch 206 training_loss_per_epoch 0.5788546204566956\n","epoch 207 training_loss_per_epoch 0.5788425207138062\n","epoch 208 training_loss_per_epoch 0.5788305401802063\n","epoch 209 training_loss_per_epoch 0.5788185000419617\n","epoch 210 training_loss_per_epoch 0.5788066983222961\n","epoch 211 training_loss_per_epoch 0.5787949562072754\n","epoch 212 training_loss_per_epoch 0.5787831544876099\n","epoch 213 training_loss_per_epoch 0.5787714123725891\n","epoch 214 training_loss_per_epoch 0.5787596702575684\n","epoch 215 training_loss_per_epoch 0.5787479281425476\n","epoch 216 training_loss_per_epoch 0.5787361860275269\n","epoch 217 training_loss_per_epoch 0.5787244439125061\n","epoch 218 training_loss_per_epoch 0.5787127017974854\n","epoch 219 training_loss_per_epoch 0.5787009596824646\n","epoch 220 training_loss_per_epoch 0.5786892771720886\n","epoch 221 training_loss_per_epoch 0.5786776542663574\n","epoch 222 training_loss_per_epoch 0.578666090965271\n","epoch 223 training_loss_per_epoch 0.5786544680595398\n","epoch 224 training_loss_per_epoch 0.5786429047584534\n","epoch 225 training_loss_per_epoch 0.5786314606666565\n","epoch 226 training_loss_per_epoch 0.578620195388794\n","epoch 227 training_loss_per_epoch 0.578609049320221\n","epoch 228 training_loss_per_epoch 0.578598141670227\n","epoch 229 training_loss_per_epoch 0.5785872936248779\n","epoch 230 training_loss_per_epoch 0.5785766243934631\n","epoch 231 training_loss_per_epoch 0.5785659551620483\n","epoch 232 training_loss_per_epoch 0.5785555243492126\n","epoch 233 training_loss_per_epoch 0.5785451531410217\n","epoch 234 training_loss_per_epoch 0.5785348415374756\n","epoch 235 training_loss_per_epoch 0.5785247087478638\n","epoch 236 training_loss_per_epoch 0.5785145163536072\n","epoch 237 training_loss_per_epoch 0.5785045027732849\n","epoch 238 training_loss_per_epoch 0.5784945487976074\n","epoch 239 training_loss_per_epoch 0.5784846544265747\n","epoch 240 training_loss_per_epoch 0.5784748196601868\n","epoch 241 training_loss_per_epoch 0.5784649848937988\n","epoch 242 training_loss_per_epoch 0.5784552693367004\n","epoch 243 training_loss_per_epoch 0.5784456133842468\n","epoch 244 training_loss_per_epoch 0.5784359574317932\n","epoch 245 training_loss_per_epoch 0.5784264802932739\n","epoch 246 training_loss_per_epoch 0.5784169435501099\n","epoch 247 training_loss_per_epoch 0.5784074664115906\n","epoch 248 training_loss_per_epoch 0.5783980488777161\n","epoch 249 training_loss_per_epoch 0.5783885717391968\n","epoch 250 training_loss_per_epoch 0.5783792734146118\n","epoch 251 training_loss_per_epoch 0.5783699750900269\n","epoch 252 training_loss_per_epoch 0.5783607363700867\n","epoch 253 training_loss_per_epoch 0.5783514380455017\n","epoch 254 training_loss_per_epoch 0.5783421397209167\n","epoch 255 training_loss_per_epoch 0.5783329606056213\n","epoch 256 training_loss_per_epoch 0.5783236622810364\n","epoch 257 training_loss_per_epoch 0.578314483165741\n","epoch 258 training_loss_per_epoch 0.5783052444458008\n","epoch 259 training_loss_per_epoch 0.5782960653305054\n","epoch 260 training_loss_per_epoch 0.5782867670059204\n","epoch 261 training_loss_per_epoch 0.5782774686813354\n","epoch 262 training_loss_per_epoch 0.5782681703567505\n","epoch 263 training_loss_per_epoch 0.5782589912414551\n","epoch 264 training_loss_per_epoch 0.5782496929168701\n","epoch 265 training_loss_per_epoch 0.5782404541969299\n","epoch 266 training_loss_per_epoch 0.5782312154769897\n","epoch 267 training_loss_per_epoch 0.5782220363616943\n","epoch 268 training_loss_per_epoch 0.5782127976417542\n","epoch 269 training_loss_per_epoch 0.5782036185264587\n","epoch 270 training_loss_per_epoch 0.5781944990158081\n","epoch 271 training_loss_per_epoch 0.5781853199005127\n","epoch 272 training_loss_per_epoch 0.5781761407852173\n","epoch 273 training_loss_per_epoch 0.5781669616699219\n","epoch 274 training_loss_per_epoch 0.5781577825546265\n","epoch 275 training_loss_per_epoch 0.5781486630439758\n","epoch 276 training_loss_per_epoch 0.5781394839286804\n","epoch 277 training_loss_per_epoch 0.5781303644180298\n","epoch 278 training_loss_per_epoch 0.5781213045120239\n","epoch 279 training_loss_per_epoch 0.5781123042106628\n","epoch 280 training_loss_per_epoch 0.5781033635139465\n","epoch 281 training_loss_per_epoch 0.578094482421875\n","epoch 282 training_loss_per_epoch 0.578085720539093\n","epoch 283 training_loss_per_epoch 0.578076958656311\n","epoch 284 training_loss_per_epoch 0.5780683159828186\n","epoch 285 training_loss_per_epoch 0.578059732913971\n","epoch 286 training_loss_per_epoch 0.5780511498451233\n","epoch 287 training_loss_per_epoch 0.5780426263809204\n","epoch 288 training_loss_per_epoch 0.5780342221260071\n","epoch 289 training_loss_per_epoch 0.5780258774757385\n","epoch 290 training_loss_per_epoch 0.5780175924301147\n","epoch 291 training_loss_per_epoch 0.5780093669891357\n","epoch 292 training_loss_per_epoch 0.5780012011528015\n","epoch 293 training_loss_per_epoch 0.5779930949211121\n","epoch 294 training_loss_per_epoch 0.5779849290847778\n","epoch 295 training_loss_per_epoch 0.5779769420623779\n","epoch 296 training_loss_per_epoch 0.5779690146446228\n","epoch 297 training_loss_per_epoch 0.5779610872268677\n","epoch 298 training_loss_per_epoch 0.5779532194137573\n","epoch 299 training_loss_per_epoch 0.5779454708099365\n","epoch 300 training_loss_per_epoch 0.577937662601471\n","epoch 301 training_loss_per_epoch 0.5779300332069397\n","epoch 302 training_loss_per_epoch 0.5779224038124084\n","epoch 303 training_loss_per_epoch 0.5779148936271667\n","epoch 304 training_loss_per_epoch 0.5779073238372803\n","epoch 305 training_loss_per_epoch 0.5778998136520386\n","epoch 306 training_loss_per_epoch 0.5778923630714417\n","epoch 307 training_loss_per_epoch 0.5778849124908447\n","epoch 308 training_loss_per_epoch 0.5778775215148926\n","epoch 309 training_loss_per_epoch 0.5778701901435852\n","epoch 310 training_loss_per_epoch 0.5778628587722778\n","epoch 311 training_loss_per_epoch 0.5778555274009705\n","epoch 312 training_loss_per_epoch 0.5778482556343079\n","epoch 313 training_loss_per_epoch 0.57784104347229\n","epoch 314 training_loss_per_epoch 0.577833890914917\n","epoch 315 training_loss_per_epoch 0.577826738357544\n","epoch 316 training_loss_per_epoch 0.5778195261955261\n","epoch 317 training_loss_per_epoch 0.5778123736381531\n","epoch 318 training_loss_per_epoch 0.5778051614761353\n","epoch 319 training_loss_per_epoch 0.5777979493141174\n","epoch 320 training_loss_per_epoch 0.5777906775474548\n","epoch 321 training_loss_per_epoch 0.5777835249900818\n","epoch 322 training_loss_per_epoch 0.5777763724327087\n","epoch 323 training_loss_per_epoch 0.57776939868927\n","epoch 324 training_loss_per_epoch 0.5777624249458313\n","epoch 325 training_loss_per_epoch 0.5777554512023926\n","epoch 326 training_loss_per_epoch 0.5777485966682434\n","epoch 327 training_loss_per_epoch 0.577741801738739\n","epoch 328 training_loss_per_epoch 0.5777350664138794\n","epoch 329 training_loss_per_epoch 0.5777283310890198\n","epoch 330 training_loss_per_epoch 0.5777215957641602\n","epoch 331 training_loss_per_epoch 0.5777149200439453\n","epoch 332 training_loss_per_epoch 0.5777082443237305\n","epoch 333 training_loss_per_epoch 0.5777015686035156\n","epoch 334 training_loss_per_epoch 0.5776948928833008\n","epoch 335 training_loss_per_epoch 0.5776882171630859\n","epoch 336 training_loss_per_epoch 0.5776816010475159\n","epoch 337 training_loss_per_epoch 0.5776749849319458\n","epoch 338 training_loss_per_epoch 0.5776683688163757\n","epoch 339 training_loss_per_epoch 0.5776616930961609\n","epoch 340 training_loss_per_epoch 0.5776551365852356\n","epoch 341 training_loss_per_epoch 0.5776485204696655\n","epoch 342 training_loss_per_epoch 0.5776419639587402\n","epoch 343 training_loss_per_epoch 0.5776354074478149\n","epoch 344 training_loss_per_epoch 0.5776289105415344\n","epoch 345 training_loss_per_epoch 0.5776224136352539\n","epoch 346 training_loss_per_epoch 0.5776159167289734\n","epoch 347 training_loss_per_epoch 0.5776094794273376\n","epoch 348 training_loss_per_epoch 0.5776030421257019\n","epoch 349 training_loss_per_epoch 0.5775967240333557\n","epoch 350 training_loss_per_epoch 0.5775902271270752\n","epoch 351 training_loss_per_epoch 0.5775838494300842\n","epoch 352 training_loss_per_epoch 0.5775774121284485\n","epoch 353 training_loss_per_epoch 0.5775709748268127\n","epoch 354 training_loss_per_epoch 0.577564537525177\n","epoch 355 training_loss_per_epoch 0.577558159828186\n","epoch 356 training_loss_per_epoch 0.5775517225265503\n","epoch 357 training_loss_per_epoch 0.5775452852249146\n","epoch 358 training_loss_per_epoch 0.5775388479232788\n","epoch 359 training_loss_per_epoch 0.5775324106216431\n","epoch 360 training_loss_per_epoch 0.5775259733200073\n","epoch 361 training_loss_per_epoch 0.5775195360183716\n","epoch 362 training_loss_per_epoch 0.5775132179260254\n","epoch 363 training_loss_per_epoch 0.5775067210197449\n","epoch 364 training_loss_per_epoch 0.5775004029273987\n","epoch 365 training_loss_per_epoch 0.5774940252304077\n","epoch 366 training_loss_per_epoch 0.5774876475334167\n","epoch 367 training_loss_per_epoch 0.5774812698364258\n","epoch 368 training_loss_per_epoch 0.57747483253479\n","epoch 369 training_loss_per_epoch 0.5774683952331543\n","epoch 370 training_loss_per_epoch 0.5774620175361633\n","epoch 371 training_loss_per_epoch 0.5774556398391724\n","epoch 372 training_loss_per_epoch 0.5774493217468262\n","epoch 373 training_loss_per_epoch 0.5774430632591248\n","epoch 374 training_loss_per_epoch 0.5774368643760681\n","epoch 375 training_loss_per_epoch 0.5774306654930115\n","epoch 376 training_loss_per_epoch 0.5774243474006653\n","epoch 377 training_loss_per_epoch 0.5774182081222534\n","epoch 378 training_loss_per_epoch 0.5774120092391968\n","epoch 379 training_loss_per_epoch 0.5774058699607849\n","epoch 380 training_loss_per_epoch 0.577399730682373\n","epoch 381 training_loss_per_epoch 0.5773937106132507\n","epoch 382 training_loss_per_epoch 0.5773876309394836\n","epoch 383 training_loss_per_epoch 0.5773816108703613\n","epoch 384 training_loss_per_epoch 0.5773756504058838\n","epoch 385 training_loss_per_epoch 0.5773696899414062\n","epoch 386 training_loss_per_epoch 0.5773637294769287\n","epoch 387 training_loss_per_epoch 0.5773577690124512\n","epoch 388 training_loss_per_epoch 0.5773518681526184\n","epoch 389 training_loss_per_epoch 0.5773459672927856\n","epoch 390 training_loss_per_epoch 0.5773400664329529\n","epoch 391 training_loss_per_epoch 0.5773341655731201\n","epoch 392 training_loss_per_epoch 0.5773283839225769\n","epoch 393 training_loss_per_epoch 0.5773225426673889\n","epoch 394 training_loss_per_epoch 0.5773168206214905\n","epoch 395 training_loss_per_epoch 0.5773110389709473\n","epoch 396 training_loss_per_epoch 0.5773053169250488\n","epoch 397 training_loss_per_epoch 0.5772996544837952\n","epoch 398 training_loss_per_epoch 0.5772939920425415\n","epoch 399 training_loss_per_epoch 0.5772883296012878\n","epoch 400 training_loss_per_epoch 0.577282726764679\n","epoch 401 training_loss_per_epoch 0.5772771239280701\n","epoch 402 training_loss_per_epoch 0.577271580696106\n","epoch 403 training_loss_per_epoch 0.5772659778594971\n","epoch 404 training_loss_per_epoch 0.5772603750228882\n","epoch 405 training_loss_per_epoch 0.5772547721862793\n","epoch 406 training_loss_per_epoch 0.5772491693496704\n","epoch 407 training_loss_per_epoch 0.5772435069084167\n","epoch 408 training_loss_per_epoch 0.5772379636764526\n","epoch 409 training_loss_per_epoch 0.5772323608398438\n","epoch 410 training_loss_per_epoch 0.5772267580032349\n","epoch 411 training_loss_per_epoch 0.577221155166626\n","epoch 412 training_loss_per_epoch 0.5772155523300171\n","epoch 413 training_loss_per_epoch 0.5772099494934082\n","epoch 414 training_loss_per_epoch 0.5772044062614441\n","epoch 415 training_loss_per_epoch 0.5771988034248352\n","epoch 416 training_loss_per_epoch 0.5771932601928711\n","epoch 417 training_loss_per_epoch 0.577187716960907\n","epoch 418 training_loss_per_epoch 0.5771822333335876\n","epoch 419 training_loss_per_epoch 0.5771768093109131\n","epoch 420 training_loss_per_epoch 0.5771713852882385\n","epoch 421 training_loss_per_epoch 0.5771658420562744\n","epoch 422 training_loss_per_epoch 0.5771604180335999\n","epoch 423 training_loss_per_epoch 0.5771549344062805\n","epoch 424 training_loss_per_epoch 0.577149510383606\n","epoch 425 training_loss_per_epoch 0.5771440267562866\n","epoch 426 training_loss_per_epoch 0.5771386027336121\n","epoch 427 training_loss_per_epoch 0.5771331787109375\n","epoch 428 training_loss_per_epoch 0.5771277546882629\n","epoch 429 training_loss_per_epoch 0.5771223306655884\n","epoch 430 training_loss_per_epoch 0.5771169662475586\n","epoch 431 training_loss_per_epoch 0.5771116018295288\n","epoch 432 training_loss_per_epoch 0.577106237411499\n","epoch 433 training_loss_per_epoch 0.5771008729934692\n","epoch 434 training_loss_per_epoch 0.5770955681800842\n","epoch 435 training_loss_per_epoch 0.5770902633666992\n","epoch 436 training_loss_per_epoch 0.5770848393440247\n","epoch 437 training_loss_per_epoch 0.5770794749259949\n","epoch 438 training_loss_per_epoch 0.5770741701126099\n","epoch 439 training_loss_per_epoch 0.5770688056945801\n","epoch 440 training_loss_per_epoch 0.5770635008811951\n","epoch 441 training_loss_per_epoch 0.5770581960678101\n","epoch 442 training_loss_per_epoch 0.5770528316497803\n","epoch 443 training_loss_per_epoch 0.5770474672317505\n","epoch 444 training_loss_per_epoch 0.5770421028137207\n","epoch 445 training_loss_per_epoch 0.5770367383956909\n","epoch 446 training_loss_per_epoch 0.5770314931869507\n","epoch 447 training_loss_per_epoch 0.5770261883735657\n","epoch 448 training_loss_per_epoch 0.5770209431648254\n","epoch 449 training_loss_per_epoch 0.57701575756073\n","epoch 450 training_loss_per_epoch 0.5770105719566345\n","epoch 451 training_loss_per_epoch 0.5770053863525391\n","epoch 452 training_loss_per_epoch 0.5770002603530884\n","epoch 453 training_loss_per_epoch 0.5769951343536377\n","epoch 454 training_loss_per_epoch 0.576990008354187\n","epoch 455 training_loss_per_epoch 0.5769848823547363\n","epoch 456 training_loss_per_epoch 0.5769797563552856\n","epoch 457 training_loss_per_epoch 0.576974630355835\n","epoch 458 training_loss_per_epoch 0.5769696235656738\n","epoch 459 training_loss_per_epoch 0.5769645571708679\n","epoch 460 training_loss_per_epoch 0.576959490776062\n","epoch 461 training_loss_per_epoch 0.5769544839859009\n","epoch 462 training_loss_per_epoch 0.576949417591095\n","epoch 463 training_loss_per_epoch 0.5769444704055786\n","epoch 464 training_loss_per_epoch 0.5769394636154175\n","epoch 465 training_loss_per_epoch 0.5769345164299011\n","epoch 466 training_loss_per_epoch 0.5769295692443848\n","epoch 467 training_loss_per_epoch 0.5769246220588684\n","epoch 468 training_loss_per_epoch 0.5769197344779968\n","epoch 469 training_loss_per_epoch 0.5769147872924805\n","epoch 470 training_loss_per_epoch 0.5769098997116089\n","epoch 471 training_loss_per_epoch 0.5769049525260925\n","epoch 472 training_loss_per_epoch 0.5769000053405762\n","epoch 473 training_loss_per_epoch 0.5768951177597046\n","epoch 474 training_loss_per_epoch 0.576890230178833\n","epoch 475 training_loss_per_epoch 0.5768853425979614\n","epoch 476 training_loss_per_epoch 0.5768804550170898\n","epoch 477 training_loss_per_epoch 0.576875627040863\n","epoch 478 training_loss_per_epoch 0.5768709182739258\n","epoch 479 training_loss_per_epoch 0.5768661499023438\n","epoch 480 training_loss_per_epoch 0.5768613815307617\n","epoch 481 training_loss_per_epoch 0.5768566131591797\n","epoch 482 training_loss_per_epoch 0.5768519043922424\n","epoch 483 training_loss_per_epoch 0.57684725522995\n","epoch 484 training_loss_per_epoch 0.5768424868583679\n","epoch 485 training_loss_per_epoch 0.5768378376960754\n","epoch 486 training_loss_per_epoch 0.576833188533783\n","epoch 487 training_loss_per_epoch 0.5768285393714905\n","epoch 488 training_loss_per_epoch 0.576823890209198\n","epoch 489 training_loss_per_epoch 0.5768192410469055\n","epoch 490 training_loss_per_epoch 0.5768147110939026\n","epoch 491 training_loss_per_epoch 0.5768101215362549\n","epoch 492 training_loss_per_epoch 0.576805591583252\n","epoch 493 training_loss_per_epoch 0.5768010020256042\n","epoch 494 training_loss_per_epoch 0.5767964720726013\n","epoch 495 training_loss_per_epoch 0.5767919421195984\n","epoch 496 training_loss_per_epoch 0.5767874121665955\n","epoch 497 training_loss_per_epoch 0.5767829418182373\n","epoch 498 training_loss_per_epoch 0.5767784714698792\n","epoch 499 training_loss_per_epoch 0.576774001121521\n","epoch 500 training_loss_per_epoch 0.5767695307731628\n","epoch 501 training_loss_per_epoch 0.5767651200294495\n","epoch 502 training_loss_per_epoch 0.5767607092857361\n","epoch 503 training_loss_per_epoch 0.5767562985420227\n","epoch 504 training_loss_per_epoch 0.5767518281936646\n","epoch 505 training_loss_per_epoch 0.5767474174499512\n","epoch 506 training_loss_per_epoch 0.5767430067062378\n","epoch 507 training_loss_per_epoch 0.5767385363578796\n","epoch 508 training_loss_per_epoch 0.576734185218811\n","epoch 509 training_loss_per_epoch 0.5767297148704529\n","epoch 510 training_loss_per_epoch 0.5767252445220947\n","epoch 511 training_loss_per_epoch 0.5767208337783813\n","epoch 512 training_loss_per_epoch 0.5767163634300232\n","epoch 513 training_loss_per_epoch 0.5767120122909546\n","epoch 514 training_loss_per_epoch 0.5767077207565308\n","epoch 515 training_loss_per_epoch 0.5767033100128174\n","epoch 516 training_loss_per_epoch 0.5766988396644592\n","epoch 517 training_loss_per_epoch 0.5766944885253906\n","epoch 518 training_loss_per_epoch 0.5766900777816772\n","epoch 519 training_loss_per_epoch 0.5766856670379639\n","epoch 520 training_loss_per_epoch 0.5766812562942505\n","epoch 521 training_loss_per_epoch 0.5766768455505371\n","epoch 522 training_loss_per_epoch 0.5766725540161133\n","epoch 523 training_loss_per_epoch 0.5766682028770447\n","epoch 524 training_loss_per_epoch 0.5766638517379761\n","epoch 525 training_loss_per_epoch 0.5766595602035522\n","epoch 526 training_loss_per_epoch 0.5766552686691284\n","epoch 527 training_loss_per_epoch 0.5766510367393494\n","epoch 528 training_loss_per_epoch 0.5766466856002808\n","epoch 529 training_loss_per_epoch 0.5766423940658569\n","epoch 530 training_loss_per_epoch 0.5766381025314331\n","epoch 531 training_loss_per_epoch 0.576633870601654\n","epoch 532 training_loss_per_epoch 0.576629638671875\n","epoch 533 training_loss_per_epoch 0.576625406742096\n","epoch 534 training_loss_per_epoch 0.5766212940216064\n","epoch 535 training_loss_per_epoch 0.5766171813011169\n","epoch 536 training_loss_per_epoch 0.576613187789917\n","epoch 537 training_loss_per_epoch 0.576609194278717\n","epoch 538 training_loss_per_epoch 0.5766052603721619\n","epoch 539 training_loss_per_epoch 0.5766013264656067\n","epoch 540 training_loss_per_epoch 0.5765973925590515\n","epoch 541 training_loss_per_epoch 0.5765934586524963\n","epoch 542 training_loss_per_epoch 0.5765895843505859\n","epoch 543 training_loss_per_epoch 0.5765857100486755\n","epoch 544 training_loss_per_epoch 0.5765818357467651\n","epoch 545 training_loss_per_epoch 0.5765780210494995\n","epoch 546 training_loss_per_epoch 0.5765742063522339\n","epoch 547 training_loss_per_epoch 0.5765703916549683\n","epoch 548 training_loss_per_epoch 0.5765665769577026\n","epoch 549 training_loss_per_epoch 0.5765628218650818\n","epoch 550 training_loss_per_epoch 0.5765590667724609\n","epoch 551 training_loss_per_epoch 0.5765553712844849\n","epoch 552 training_loss_per_epoch 0.576551616191864\n","epoch 553 training_loss_per_epoch 0.5765479207038879\n","epoch 554 training_loss_per_epoch 0.5765442848205566\n","epoch 555 training_loss_per_epoch 0.5765405297279358\n","epoch 556 training_loss_per_epoch 0.5765368938446045\n","epoch 557 training_loss_per_epoch 0.576533317565918\n","epoch 558 training_loss_per_epoch 0.5765296816825867\n","epoch 559 training_loss_per_epoch 0.5765261054039001\n","epoch 560 training_loss_per_epoch 0.5765224695205688\n","epoch 561 training_loss_per_epoch 0.5765189528465271\n","epoch 562 training_loss_per_epoch 0.5765154361724854\n","epoch 563 training_loss_per_epoch 0.5765119194984436\n","epoch 564 training_loss_per_epoch 0.5765083432197571\n","epoch 565 training_loss_per_epoch 0.5765048861503601\n","epoch 566 training_loss_per_epoch 0.5765014290809631\n","epoch 567 training_loss_per_epoch 0.5764979720115662\n","epoch 568 training_loss_per_epoch 0.576494574546814\n","epoch 569 training_loss_per_epoch 0.576491117477417\n","epoch 570 training_loss_per_epoch 0.5764877200126648\n","epoch 571 training_loss_per_epoch 0.5764842629432678\n","epoch 572 training_loss_per_epoch 0.5764808654785156\n","epoch 573 training_loss_per_epoch 0.5764775276184082\n","epoch 574 training_loss_per_epoch 0.576474130153656\n","epoch 575 training_loss_per_epoch 0.5764708518981934\n","epoch 576 training_loss_per_epoch 0.5764675736427307\n","epoch 577 training_loss_per_epoch 0.5764642357826233\n","epoch 578 training_loss_per_epoch 0.5764609575271606\n","epoch 579 training_loss_per_epoch 0.5764577388763428\n","epoch 580 training_loss_per_epoch 0.5764544010162354\n","epoch 581 training_loss_per_epoch 0.5764511823654175\n","epoch 582 training_loss_per_epoch 0.5764479637145996\n","epoch 583 training_loss_per_epoch 0.576444685459137\n","epoch 584 training_loss_per_epoch 0.5764415860176086\n","epoch 585 training_loss_per_epoch 0.5764384269714355\n","epoch 586 training_loss_per_epoch 0.5764352083206177\n","epoch 587 training_loss_per_epoch 0.5764320492744446\n","epoch 588 training_loss_per_epoch 0.5764289498329163\n","epoch 589 training_loss_per_epoch 0.5764258503913879\n","epoch 590 training_loss_per_epoch 0.5764227509498596\n","epoch 591 training_loss_per_epoch 0.5764196515083313\n","epoch 592 training_loss_per_epoch 0.576416552066803\n","epoch 593 training_loss_per_epoch 0.5764134526252747\n","epoch 594 training_loss_per_epoch 0.5764103531837463\n","epoch 595 training_loss_per_epoch 0.576407253742218\n","epoch 596 training_loss_per_epoch 0.5764040946960449\n","epoch 597 training_loss_per_epoch 0.5764009356498718\n","epoch 598 training_loss_per_epoch 0.5763978362083435\n","epoch 599 training_loss_per_epoch 0.5763946771621704\n","epoch 600 training_loss_per_epoch 0.5763914585113525\n","epoch 601 training_loss_per_epoch 0.5763881802558899\n","epoch 602 training_loss_per_epoch 0.5763849020004272\n","epoch 603 training_loss_per_epoch 0.5763815641403198\n","epoch 604 training_loss_per_epoch 0.5763782858848572\n","epoch 605 training_loss_per_epoch 0.5763750672340393\n","epoch 606 training_loss_per_epoch 0.5763717293739319\n","epoch 607 training_loss_per_epoch 0.576368510723114\n","epoch 608 training_loss_per_epoch 0.5763652920722961\n","epoch 609 training_loss_per_epoch 0.5763621926307678\n","epoch 610 training_loss_per_epoch 0.5763592720031738\n","epoch 611 training_loss_per_epoch 0.5763562917709351\n","epoch 612 training_loss_per_epoch 0.5763533115386963\n","epoch 613 training_loss_per_epoch 0.5763503313064575\n","epoch 614 training_loss_per_epoch 0.5763474106788635\n","epoch 615 training_loss_per_epoch 0.5763444900512695\n","epoch 616 training_loss_per_epoch 0.5763415694236755\n","epoch 617 training_loss_per_epoch 0.5763387084007263\n","epoch 618 training_loss_per_epoch 0.5763358473777771\n","epoch 619 training_loss_per_epoch 0.5763329863548279\n","epoch 620 training_loss_per_epoch 0.5763301849365234\n","epoch 621 training_loss_per_epoch 0.5763273239135742\n","epoch 622 training_loss_per_epoch 0.5763245224952698\n","epoch 623 training_loss_per_epoch 0.5763217806816101\n","epoch 624 training_loss_per_epoch 0.5763190388679504\n","epoch 625 training_loss_per_epoch 0.576316237449646\n","epoch 626 training_loss_per_epoch 0.5763134956359863\n","epoch 627 training_loss_per_epoch 0.5763107538223267\n","epoch 628 training_loss_per_epoch 0.576308012008667\n","epoch 629 training_loss_per_epoch 0.5763052701950073\n","epoch 630 training_loss_per_epoch 0.5763025879859924\n","epoch 631 training_loss_per_epoch 0.5762999057769775\n","epoch 632 training_loss_per_epoch 0.5762972235679626\n","epoch 633 training_loss_per_epoch 0.5762946009635925\n","epoch 634 training_loss_per_epoch 0.5762919187545776\n","epoch 635 training_loss_per_epoch 0.5762893557548523\n","epoch 636 training_loss_per_epoch 0.576286792755127\n","epoch 637 training_loss_per_epoch 0.5762841701507568\n","epoch 638 training_loss_per_epoch 0.5762816071510315\n","epoch 639 training_loss_per_epoch 0.5762790441513062\n","epoch 640 training_loss_per_epoch 0.5762764811515808\n","epoch 641 training_loss_per_epoch 0.5762739777565002\n","epoch 642 training_loss_per_epoch 0.5762714147567749\n","epoch 643 training_loss_per_epoch 0.5762688517570496\n","epoch 644 training_loss_per_epoch 0.5762662887573242\n","epoch 645 training_loss_per_epoch 0.5762637853622437\n","epoch 646 training_loss_per_epoch 0.5762612819671631\n","epoch 647 training_loss_per_epoch 0.5762587785720825\n","epoch 648 training_loss_per_epoch 0.576256275177002\n","epoch 649 training_loss_per_epoch 0.5762537717819214\n","epoch 650 training_loss_per_epoch 0.5762513875961304\n","epoch 651 training_loss_per_epoch 0.5762489438056946\n","epoch 652 training_loss_per_epoch 0.5762465000152588\n","epoch 653 training_loss_per_epoch 0.576244056224823\n","epoch 654 training_loss_per_epoch 0.576241672039032\n","epoch 655 training_loss_per_epoch 0.5762392282485962\n","epoch 656 training_loss_per_epoch 0.5762367844581604\n","epoch 657 training_loss_per_epoch 0.5762344002723694\n","epoch 658 training_loss_per_epoch 0.5762319564819336\n","epoch 659 training_loss_per_epoch 0.5762296319007874\n","epoch 660 training_loss_per_epoch 0.5762272477149963\n","epoch 661 training_loss_per_epoch 0.5762249231338501\n","epoch 662 training_loss_per_epoch 0.5762226581573486\n","epoch 663 training_loss_per_epoch 0.5762202739715576\n","epoch 664 training_loss_per_epoch 0.5762180089950562\n","epoch 665 training_loss_per_epoch 0.5762156844139099\n","epoch 666 training_loss_per_epoch 0.5762134194374084\n","epoch 667 training_loss_per_epoch 0.5762110948562622\n","epoch 668 training_loss_per_epoch 0.5762088298797607\n","epoch 669 training_loss_per_epoch 0.5762065052986145\n","epoch 670 training_loss_per_epoch 0.5762042999267578\n","epoch 671 training_loss_per_epoch 0.5762020349502563\n","epoch 672 training_loss_per_epoch 0.5761997699737549\n","epoch 673 training_loss_per_epoch 0.5761975049972534\n","epoch 674 training_loss_per_epoch 0.5761953592300415\n","epoch 675 training_loss_per_epoch 0.5761931538581848\n","epoch 676 training_loss_per_epoch 0.5761909484863281\n","epoch 677 training_loss_per_epoch 0.5761886835098267\n","epoch 678 training_loss_per_epoch 0.57618647813797\n","epoch 679 training_loss_per_epoch 0.5761842727661133\n","epoch 680 training_loss_per_epoch 0.5761821269989014\n","epoch 681 training_loss_per_epoch 0.5761799216270447\n","epoch 682 training_loss_per_epoch 0.5761777758598328\n","epoch 683 training_loss_per_epoch 0.5761756896972656\n","epoch 684 training_loss_per_epoch 0.5761736035346985\n","epoch 685 training_loss_per_epoch 0.5761714577674866\n","epoch 686 training_loss_per_epoch 0.5761693716049194\n","epoch 687 training_loss_per_epoch 0.5761672854423523\n","epoch 688 training_loss_per_epoch 0.5761651992797852\n","epoch 689 training_loss_per_epoch 0.576163113117218\n","epoch 690 training_loss_per_epoch 0.5761610865592957\n","epoch 691 training_loss_per_epoch 0.5761590003967285\n","epoch 692 training_loss_per_epoch 0.5761569738388062\n","epoch 693 training_loss_per_epoch 0.5761549472808838\n","epoch 694 training_loss_per_epoch 0.5761528611183167\n","epoch 695 training_loss_per_epoch 0.5761508345603943\n","epoch 696 training_loss_per_epoch 0.5761488080024719\n","epoch 697 training_loss_per_epoch 0.5761467814445496\n","epoch 698 training_loss_per_epoch 0.5761447548866272\n","epoch 699 training_loss_per_epoch 0.5761427283287048\n","epoch 700 training_loss_per_epoch 0.5761407017707825\n","epoch 701 training_loss_per_epoch 0.5761386156082153\n","epoch 702 training_loss_per_epoch 0.5761365294456482\n","epoch 703 training_loss_per_epoch 0.5761345028877258\n","epoch 704 training_loss_per_epoch 0.5761323571205139\n","epoch 705 training_loss_per_epoch 0.5761303305625916\n","epoch 706 training_loss_per_epoch 0.5761283040046692\n","epoch 707 training_loss_per_epoch 0.5761262774467468\n","epoch 708 training_loss_per_epoch 0.5761241912841797\n","epoch 709 training_loss_per_epoch 0.5761221647262573\n","epoch 710 training_loss_per_epoch 0.5761200785636902\n","epoch 711 training_loss_per_epoch 0.576117992401123\n","epoch 712 training_loss_per_epoch 0.5761159062385559\n","epoch 713 training_loss_per_epoch 0.576113760471344\n","epoch 714 training_loss_per_epoch 0.5761116147041321\n","epoch 715 training_loss_per_epoch 0.5761094689369202\n","epoch 716 training_loss_per_epoch 0.576107382774353\n","epoch 717 training_loss_per_epoch 0.5761052966117859\n","epoch 718 training_loss_per_epoch 0.576103150844574\n","epoch 719 training_loss_per_epoch 0.5761009454727173\n","epoch 720 training_loss_per_epoch 0.5760987997055054\n","epoch 721 training_loss_per_epoch 0.5760965943336487\n","epoch 722 training_loss_per_epoch 0.5760944485664368\n","epoch 723 training_loss_per_epoch 0.5760923027992249\n","epoch 724 training_loss_per_epoch 0.5760900974273682\n","epoch 725 training_loss_per_epoch 0.5760878920555115\n","epoch 726 training_loss_per_epoch 0.57608562707901\n","epoch 727 training_loss_per_epoch 0.5760833621025085\n","epoch 728 training_loss_per_epoch 0.5760810375213623\n","epoch 729 training_loss_per_epoch 0.5760787129402161\n","epoch 730 training_loss_per_epoch 0.5760764479637146\n","epoch 731 training_loss_per_epoch 0.5760741829872131\n","epoch 732 training_loss_per_epoch 0.5760719776153564\n","epoch 733 training_loss_per_epoch 0.576069712638855\n","epoch 734 training_loss_per_epoch 0.5760675072669983\n","epoch 735 training_loss_per_epoch 0.5760653018951416\n","epoch 736 training_loss_per_epoch 0.5760631561279297\n","epoch 737 training_loss_per_epoch 0.5760610699653625\n","epoch 738 training_loss_per_epoch 0.5760589241981506\n","epoch 739 training_loss_per_epoch 0.5760568380355835\n","epoch 740 training_loss_per_epoch 0.5760547518730164\n","epoch 741 training_loss_per_epoch 0.5760526657104492\n","epoch 742 training_loss_per_epoch 0.5760505795478821\n","epoch 743 training_loss_per_epoch 0.5760485529899597\n","epoch 744 training_loss_per_epoch 0.5760465264320374\n","epoch 745 training_loss_per_epoch 0.576044499874115\n","epoch 746 training_loss_per_epoch 0.5760424733161926\n","epoch 747 training_loss_per_epoch 0.5760404467582703\n","epoch 748 training_loss_per_epoch 0.5760383605957031\n","epoch 749 training_loss_per_epoch 0.5760363340377808\n","epoch 750 training_loss_per_epoch 0.5760341882705688\n","epoch 751 training_loss_per_epoch 0.5760320425033569\n","epoch 752 training_loss_per_epoch 0.5760297775268555\n","epoch 753 training_loss_per_epoch 0.5760274529457092\n","epoch 754 training_loss_per_epoch 0.5760251879692078\n","epoch 755 training_loss_per_epoch 0.5760228633880615\n","epoch 756 training_loss_per_epoch 0.5760205388069153\n","epoch 757 training_loss_per_epoch 0.576018214225769\n","epoch 758 training_loss_per_epoch 0.5760160088539124\n","epoch 759 training_loss_per_epoch 0.5760138034820557\n","epoch 760 training_loss_per_epoch 0.5760115385055542\n","epoch 761 training_loss_per_epoch 0.5760093331336975\n","epoch 762 training_loss_per_epoch 0.5760070085525513\n","epoch 763 training_loss_per_epoch 0.5760048031806946\n","epoch 764 training_loss_per_epoch 0.5760024785995483\n","epoch 765 training_loss_per_epoch 0.5760000944137573\n","epoch 766 training_loss_per_epoch 0.5759976506233215\n","epoch 767 training_loss_per_epoch 0.5759952068328857\n","epoch 768 training_loss_per_epoch 0.5759926438331604\n","epoch 769 training_loss_per_epoch 0.5759899616241455\n","epoch 770 training_loss_per_epoch 0.5759872794151306\n","epoch 771 training_loss_per_epoch 0.5759844183921814\n","epoch 772 training_loss_per_epoch 0.5759815573692322\n","epoch 773 training_loss_per_epoch 0.5759787559509277\n","epoch 774 training_loss_per_epoch 0.5759758949279785\n","epoch 775 training_loss_per_epoch 0.5759729146957397\n","epoch 776 training_loss_per_epoch 0.575969934463501\n","epoch 777 training_loss_per_epoch 0.5759665966033936\n","epoch 778 training_loss_per_epoch 0.5759634375572205\n","epoch 779 training_loss_per_epoch 0.5759602189064026\n","epoch 780 training_loss_per_epoch 0.5759570002555847\n","epoch 781 training_loss_per_epoch 0.5759536623954773\n","epoch 782 training_loss_per_epoch 0.5759505033493042\n","epoch 783 training_loss_per_epoch 0.5759472846984863\n","epoch 784 training_loss_per_epoch 0.5759439468383789\n","epoch 785 training_loss_per_epoch 0.5759406089782715\n","epoch 786 training_loss_per_epoch 0.5759371519088745\n","epoch 787 training_loss_per_epoch 0.5759338736534119\n","epoch 788 training_loss_per_epoch 0.5759304761886597\n","epoch 789 training_loss_per_epoch 0.5759269595146179\n","epoch 790 training_loss_per_epoch 0.5759233832359314\n","epoch 791 training_loss_per_epoch 0.5759198665618896\n","epoch 792 training_loss_per_epoch 0.5759164094924927\n","epoch 793 training_loss_per_epoch 0.5759130120277405\n","epoch 794 training_loss_per_epoch 0.5759097337722778\n","epoch 795 training_loss_per_epoch 0.5759064555168152\n","epoch 796 training_loss_per_epoch 0.5759032368659973\n","epoch 797 training_loss_per_epoch 0.5758998990058899\n","epoch 798 training_loss_per_epoch 0.5758965015411377\n","epoch 799 training_loss_per_epoch 0.5758931636810303\n","epoch 800 training_loss_per_epoch 0.5758897066116333\n","epoch 801 training_loss_per_epoch 0.5758863091468811\n","epoch 802 training_loss_per_epoch 0.5758830308914185\n","epoch 803 training_loss_per_epoch 0.5758796334266663\n","epoch 804 training_loss_per_epoch 0.5758761167526245\n","epoch 805 training_loss_per_epoch 0.5758723616600037\n","epoch 806 training_loss_per_epoch 0.575868546962738\n","epoch 807 training_loss_per_epoch 0.575864851474762\n","epoch 808 training_loss_per_epoch 0.5758609771728516\n","epoch 809 training_loss_per_epoch 0.5758569836616516\n","epoch 810 training_loss_per_epoch 0.5758532285690308\n","epoch 811 training_loss_per_epoch 0.5758495926856995\n","epoch 812 training_loss_per_epoch 0.5758460760116577\n","epoch 813 training_loss_per_epoch 0.5758424997329712\n","epoch 814 training_loss_per_epoch 0.5758389830589294\n","epoch 815 training_loss_per_epoch 0.5758354067802429\n","epoch 816 training_loss_per_epoch 0.5758318901062012\n","epoch 817 training_loss_per_epoch 0.5758287310600281\n","epoch 818 training_loss_per_epoch 0.5758256316184998\n","epoch 819 training_loss_per_epoch 0.5758225321769714\n","epoch 820 training_loss_per_epoch 0.5758194327354431\n","epoch 821 training_loss_per_epoch 0.5758163332939148\n","epoch 822 training_loss_per_epoch 0.5758131146430969\n","epoch 823 training_loss_per_epoch 0.5758098363876343\n","epoch 824 training_loss_per_epoch 0.5758065581321716\n","epoch 825 training_loss_per_epoch 0.5758033990859985\n","epoch 826 training_loss_per_epoch 0.5758002400398254\n","epoch 827 training_loss_per_epoch 0.5757970809936523\n","epoch 828 training_loss_per_epoch 0.5757938027381897\n","epoch 829 training_loss_per_epoch 0.5757905840873718\n","epoch 830 training_loss_per_epoch 0.5757873058319092\n","epoch 831 training_loss_per_epoch 0.5757840275764465\n","epoch 832 training_loss_per_epoch 0.5757807493209839\n","epoch 833 training_loss_per_epoch 0.5757774710655212\n","epoch 834 training_loss_per_epoch 0.5757741928100586\n","epoch 835 training_loss_per_epoch 0.575770914554596\n","epoch 836 training_loss_per_epoch 0.5757676362991333\n","epoch 837 training_loss_per_epoch 0.5757644176483154\n","epoch 838 training_loss_per_epoch 0.5757611989974976\n","epoch 839 training_loss_per_epoch 0.5757579803466797\n","epoch 840 training_loss_per_epoch 0.5757546424865723\n","epoch 841 training_loss_per_epoch 0.5757513046264648\n","epoch 842 training_loss_per_epoch 0.5757481455802917\n","epoch 843 training_loss_per_epoch 0.5757449269294739\n","epoch 844 training_loss_per_epoch 0.5757416486740112\n","epoch 845 training_loss_per_epoch 0.5757383704185486\n","epoch 846 training_loss_per_epoch 0.5757349729537964\n","epoch 847 training_loss_per_epoch 0.5757315754890442\n","epoch 848 training_loss_per_epoch 0.575728178024292\n","epoch 849 training_loss_per_epoch 0.5757248401641846\n","epoch 850 training_loss_per_epoch 0.5757214426994324\n","epoch 851 training_loss_per_epoch 0.5757180452346802\n","epoch 852 training_loss_per_epoch 0.575714647769928\n","epoch 853 training_loss_per_epoch 0.575711190700531\n","epoch 854 training_loss_per_epoch 0.5757077932357788\n","epoch 855 training_loss_per_epoch 0.5757042765617371\n","epoch 856 training_loss_per_epoch 0.5757008194923401\n","epoch 857 training_loss_per_epoch 0.5756972432136536\n","epoch 858 training_loss_per_epoch 0.5756935477256775\n","epoch 859 training_loss_per_epoch 0.5756897926330566\n","epoch 860 training_loss_per_epoch 0.5756860971450806\n","epoch 861 training_loss_per_epoch 0.5756824612617493\n","epoch 862 training_loss_per_epoch 0.5756789445877075\n","epoch 863 training_loss_per_epoch 0.5756753087043762\n","epoch 864 training_loss_per_epoch 0.5756717920303345\n","epoch 865 training_loss_per_epoch 0.5756683349609375\n","epoch 866 training_loss_per_epoch 0.5756649971008301\n","epoch 867 training_loss_per_epoch 0.5756615996360779\n","epoch 868 training_loss_per_epoch 0.5756582617759705\n","epoch 869 training_loss_per_epoch 0.5756547451019287\n","epoch 870 training_loss_per_epoch 0.5756511688232422\n","epoch 871 training_loss_per_epoch 0.5756475925445557\n","epoch 872 training_loss_per_epoch 0.5756441950798035\n","epoch 873 training_loss_per_epoch 0.575640857219696\n","epoch 874 training_loss_per_epoch 0.5756374597549438\n","epoch 875 training_loss_per_epoch 0.5756340622901917\n","epoch 876 training_loss_per_epoch 0.5756305456161499\n","epoch 877 training_loss_per_epoch 0.5756270885467529\n","epoch 878 training_loss_per_epoch 0.5756233930587769\n","epoch 879 training_loss_per_epoch 0.5756195783615112\n","epoch 880 training_loss_per_epoch 0.5756157040596008\n","epoch 881 training_loss_per_epoch 0.5756117105484009\n","epoch 882 training_loss_per_epoch 0.5756078362464905\n","epoch 883 training_loss_per_epoch 0.5756039023399353\n","epoch 884 training_loss_per_epoch 0.5755999088287354\n","epoch 885 training_loss_per_epoch 0.5755959153175354\n","epoch 886 training_loss_per_epoch 0.5755918025970459\n","epoch 887 training_loss_per_epoch 0.5755875706672668\n","epoch 888 training_loss_per_epoch 0.575583279132843\n","epoch 889 training_loss_per_epoch 0.5755791068077087\n","epoch 890 training_loss_per_epoch 0.5755751132965088\n","epoch 891 training_loss_per_epoch 0.5755710601806641\n","epoch 892 training_loss_per_epoch 0.5755668878555298\n","epoch 893 training_loss_per_epoch 0.575562596321106\n","epoch 894 training_loss_per_epoch 0.5755582451820374\n","epoch 895 training_loss_per_epoch 0.5755537152290344\n","epoch 896 training_loss_per_epoch 0.5755492448806763\n","epoch 897 training_loss_per_epoch 0.5755448937416077\n","epoch 898 training_loss_per_epoch 0.5755407810211182\n","epoch 899 training_loss_per_epoch 0.5755369067192078\n","epoch 900 training_loss_per_epoch 0.5755332708358765\n","epoch 901 training_loss_per_epoch 0.5755296945571899\n","epoch 902 training_loss_per_epoch 0.5755261778831482\n","epoch 903 training_loss_per_epoch 0.575522780418396\n","epoch 904 training_loss_per_epoch 0.5755194425582886\n","epoch 905 training_loss_per_epoch 0.5755161046981812\n","epoch 906 training_loss_per_epoch 0.5755128264427185\n","epoch 907 training_loss_per_epoch 0.5755095481872559\n","epoch 908 training_loss_per_epoch 0.5755062699317932\n","epoch 909 training_loss_per_epoch 0.5755029320716858\n","epoch 910 training_loss_per_epoch 0.5754996538162231\n","epoch 911 training_loss_per_epoch 0.5754963755607605\n","epoch 912 training_loss_per_epoch 0.5754930377006531\n","epoch 913 training_loss_per_epoch 0.5754898190498352\n","epoch 914 training_loss_per_epoch 0.5754865407943726\n","epoch 915 training_loss_per_epoch 0.5754832625389099\n","epoch 916 training_loss_per_epoch 0.575480043888092\n","epoch 917 training_loss_per_epoch 0.5754768252372742\n","epoch 918 training_loss_per_epoch 0.5754736661911011\n","epoch 919 training_loss_per_epoch 0.5754704475402832\n","epoch 920 training_loss_per_epoch 0.5754673480987549\n","epoch 921 training_loss_per_epoch 0.5754642486572266\n","epoch 922 training_loss_per_epoch 0.575461208820343\n","epoch 923 training_loss_per_epoch 0.5754581689834595\n","epoch 924 training_loss_per_epoch 0.5754552483558655\n","epoch 925 training_loss_per_epoch 0.5754523277282715\n","epoch 926 training_loss_per_epoch 0.5754494667053223\n","epoch 927 training_loss_per_epoch 0.5754465460777283\n","epoch 928 training_loss_per_epoch 0.5754437446594238\n","epoch 929 training_loss_per_epoch 0.5754409432411194\n","epoch 930 training_loss_per_epoch 0.5754381418228149\n","epoch 931 training_loss_per_epoch 0.5754354000091553\n","epoch 932 training_loss_per_epoch 0.5754326581954956\n","epoch 933 training_loss_per_epoch 0.5754299759864807\n","epoch 934 training_loss_per_epoch 0.575427234172821\n","epoch 935 training_loss_per_epoch 0.5754245519638062\n","epoch 936 training_loss_per_epoch 0.5754218697547913\n","epoch 937 training_loss_per_epoch 0.5754191875457764\n","epoch 938 training_loss_per_epoch 0.5754165649414062\n","epoch 939 training_loss_per_epoch 0.5754139423370361\n","epoch 940 training_loss_per_epoch 0.5754113793373108\n","epoch 941 training_loss_per_epoch 0.5754088759422302\n","epoch 942 training_loss_per_epoch 0.5754064321517944\n","epoch 943 training_loss_per_epoch 0.5754039287567139\n","epoch 944 training_loss_per_epoch 0.5754015445709229\n","epoch 945 training_loss_per_epoch 0.5753991007804871\n","epoch 946 training_loss_per_epoch 0.5753966569900513\n","epoch 947 training_loss_per_epoch 0.5753942131996155\n","epoch 948 training_loss_per_epoch 0.5753918290138245\n","epoch 949 training_loss_per_epoch 0.5753895044326782\n","epoch 950 training_loss_per_epoch 0.5753871202468872\n","epoch 951 training_loss_per_epoch 0.575384795665741\n","epoch 952 training_loss_per_epoch 0.5753825306892395\n","epoch 953 training_loss_per_epoch 0.5753802061080933\n","epoch 954 training_loss_per_epoch 0.5753779411315918\n","epoch 955 training_loss_per_epoch 0.5753756761550903\n","epoch 956 training_loss_per_epoch 0.5753734707832336\n","epoch 957 training_loss_per_epoch 0.5753712058067322\n","epoch 958 training_loss_per_epoch 0.5753690004348755\n","epoch 959 training_loss_per_epoch 0.575366735458374\n","epoch 960 training_loss_per_epoch 0.5753645300865173\n","epoch 961 training_loss_per_epoch 0.5753623843193054\n","epoch 962 training_loss_per_epoch 0.5753602385520935\n","epoch 963 training_loss_per_epoch 0.5753580927848816\n","epoch 964 training_loss_per_epoch 0.5753560066223145\n","epoch 965 training_loss_per_epoch 0.5753539204597473\n","epoch 966 training_loss_per_epoch 0.5753517746925354\n","epoch 967 training_loss_per_epoch 0.575349748134613\n","epoch 968 training_loss_per_epoch 0.5753476023674011\n","epoch 969 training_loss_per_epoch 0.5753455758094788\n","epoch 970 training_loss_per_epoch 0.5753434896469116\n","epoch 971 training_loss_per_epoch 0.5753414034843445\n","epoch 972 training_loss_per_epoch 0.5753393769264221\n","epoch 973 training_loss_per_epoch 0.5753374099731445\n","epoch 974 training_loss_per_epoch 0.5753353834152222\n","epoch 975 training_loss_per_epoch 0.5753334164619446\n","epoch 976 training_loss_per_epoch 0.575331449508667\n","epoch 977 training_loss_per_epoch 0.5753294825553894\n","epoch 978 training_loss_per_epoch 0.5753275156021118\n","epoch 979 training_loss_per_epoch 0.575325608253479\n","epoch 980 training_loss_per_epoch 0.5753236413002014\n","epoch 981 training_loss_per_epoch 0.5753217935562134\n","epoch 982 training_loss_per_epoch 0.5753198862075806\n","epoch 983 training_loss_per_epoch 0.5753179788589478\n","epoch 984 training_loss_per_epoch 0.5753160715103149\n","epoch 985 training_loss_per_epoch 0.5753141641616821\n","epoch 986 training_loss_per_epoch 0.5753122568130493\n","epoch 987 training_loss_per_epoch 0.5753103494644165\n","epoch 988 training_loss_per_epoch 0.5753085613250732\n","epoch 989 training_loss_per_epoch 0.5753067135810852\n","epoch 990 training_loss_per_epoch 0.5753049254417419\n","epoch 991 training_loss_per_epoch 0.5753030776977539\n","epoch 992 training_loss_per_epoch 0.5753012895584106\n","epoch 993 training_loss_per_epoch 0.5752995610237122\n","epoch 994 training_loss_per_epoch 0.5752977132797241\n","epoch 995 training_loss_per_epoch 0.5752959847450256\n","epoch 996 training_loss_per_epoch 0.5752942562103271\n","epoch 997 training_loss_per_epoch 0.5752925276756287\n","epoch 998 training_loss_per_epoch 0.5752907395362854\n","epoch 999 training_loss_per_epoch 0.5752890706062317\n","epoch 1000 training_loss_per_epoch 0.5752873420715332\n","epoch 1001 training_loss_per_epoch 0.5752856731414795\n","epoch 1002 training_loss_per_epoch 0.575283944606781\n","epoch 1003 training_loss_per_epoch 0.5752822160720825\n","epoch 1004 training_loss_per_epoch 0.575280487537384\n","epoch 1005 training_loss_per_epoch 0.5752788186073303\n","epoch 1006 training_loss_per_epoch 0.5752770900726318\n","epoch 1007 training_loss_per_epoch 0.5752754807472229\n","epoch 1008 training_loss_per_epoch 0.5752738118171692\n","epoch 1009 training_loss_per_epoch 0.5752721428871155\n","epoch 1010 training_loss_per_epoch 0.5752705335617065\n","epoch 1011 training_loss_per_epoch 0.5752688646316528\n","epoch 1012 training_loss_per_epoch 0.5752672553062439\n","epoch 1013 training_loss_per_epoch 0.575265645980835\n","epoch 1014 training_loss_per_epoch 0.575264036655426\n","epoch 1015 training_loss_per_epoch 0.5752624869346619\n","epoch 1016 training_loss_per_epoch 0.5752608776092529\n","epoch 1017 training_loss_per_epoch 0.5752593278884888\n","epoch 1018 training_loss_per_epoch 0.5752577781677246\n","epoch 1019 training_loss_per_epoch 0.5752562880516052\n","epoch 1020 training_loss_per_epoch 0.5752547383308411\n","epoch 1021 training_loss_per_epoch 0.5752532482147217\n","epoch 1022 training_loss_per_epoch 0.5752517580986023\n","epoch 1023 training_loss_per_epoch 0.5752502679824829\n","epoch 1024 training_loss_per_epoch 0.5752487778663635\n","epoch 1025 training_loss_per_epoch 0.5752474069595337\n","epoch 1026 training_loss_per_epoch 0.5752459764480591\n","epoch 1027 training_loss_per_epoch 0.5752446055412292\n","epoch 1028 training_loss_per_epoch 0.5752432346343994\n","epoch 1029 training_loss_per_epoch 0.5752418637275696\n","epoch 1030 training_loss_per_epoch 0.5752404928207397\n","epoch 1031 training_loss_per_epoch 0.5752391815185547\n","epoch 1032 training_loss_per_epoch 0.5752378702163696\n","epoch 1033 training_loss_per_epoch 0.5752365589141846\n","epoch 1034 training_loss_per_epoch 0.5752352476119995\n","epoch 1035 training_loss_per_epoch 0.5752339959144592\n","epoch 1036 training_loss_per_epoch 0.5752326846122742\n","epoch 1037 training_loss_per_epoch 0.5752314329147339\n","epoch 1038 training_loss_per_epoch 0.5752301812171936\n","epoch 1039 training_loss_per_epoch 0.5752289295196533\n","epoch 1040 training_loss_per_epoch 0.575227677822113\n","epoch 1041 training_loss_per_epoch 0.5752264261245728\n","epoch 1042 training_loss_per_epoch 0.5752252340316772\n","epoch 1043 training_loss_per_epoch 0.575223982334137\n","epoch 1044 training_loss_per_epoch 0.5752228498458862\n","epoch 1045 training_loss_per_epoch 0.575221598148346\n","epoch 1046 training_loss_per_epoch 0.5752204060554504\n","epoch 1047 training_loss_per_epoch 0.5752192139625549\n","epoch 1048 training_loss_per_epoch 0.5752180218696594\n","epoch 1049 training_loss_per_epoch 0.5752168893814087\n","epoch 1050 training_loss_per_epoch 0.5752156972885132\n","epoch 1051 training_loss_per_epoch 0.5752145648002625\n","epoch 1052 training_loss_per_epoch 0.5752133727073669\n","epoch 1053 training_loss_per_epoch 0.5752122402191162\n","epoch 1054 training_loss_per_epoch 0.5752111673355103\n","epoch 1055 training_loss_per_epoch 0.5752099752426147\n","epoch 1056 training_loss_per_epoch 0.575208842754364\n","epoch 1057 training_loss_per_epoch 0.5752077102661133\n","epoch 1058 training_loss_per_epoch 0.5752065777778625\n","epoch 1059 training_loss_per_epoch 0.5752054452896118\n","epoch 1060 training_loss_per_epoch 0.5752043724060059\n","epoch 1061 training_loss_per_epoch 0.5752032399177551\n","epoch 1062 training_loss_per_epoch 0.5752021670341492\n","epoch 1063 training_loss_per_epoch 0.5752010941505432\n","epoch 1064 training_loss_per_epoch 0.5751999616622925\n","epoch 1065 training_loss_per_epoch 0.5751988887786865\n","epoch 1066 training_loss_per_epoch 0.5751978158950806\n","epoch 1067 training_loss_per_epoch 0.5751968026161194\n","epoch 1068 training_loss_per_epoch 0.5751957297325134\n","epoch 1069 training_loss_per_epoch 0.5751947164535522\n","epoch 1070 training_loss_per_epoch 0.5751937031745911\n","epoch 1071 training_loss_per_epoch 0.5751927495002747\n","epoch 1072 training_loss_per_epoch 0.5751917362213135\n","epoch 1073 training_loss_per_epoch 0.5751907229423523\n","epoch 1074 training_loss_per_epoch 0.5751897096633911\n","epoch 1075 training_loss_per_epoch 0.5751886963844299\n","epoch 1076 training_loss_per_epoch 0.5751877427101135\n","epoch 1077 training_loss_per_epoch 0.5751867890357971\n","epoch 1078 training_loss_per_epoch 0.5751857757568359\n","epoch 1079 training_loss_per_epoch 0.5751848220825195\n","epoch 1080 training_loss_per_epoch 0.5751838088035583\n","epoch 1081 training_loss_per_epoch 0.5751828551292419\n","epoch 1082 training_loss_per_epoch 0.5751818418502808\n","epoch 1083 training_loss_per_epoch 0.5751808881759644\n","epoch 1084 training_loss_per_epoch 0.575179934501648\n","epoch 1085 training_loss_per_epoch 0.5751790404319763\n","epoch 1086 training_loss_per_epoch 0.5751780867576599\n","epoch 1087 training_loss_per_epoch 0.5751771330833435\n","epoch 1088 training_loss_per_epoch 0.5751761794090271\n","epoch 1089 training_loss_per_epoch 0.5751752853393555\n","epoch 1090 training_loss_per_epoch 0.5751742720603943\n","epoch 1091 training_loss_per_epoch 0.5751733779907227\n","epoch 1092 training_loss_per_epoch 0.575172483921051\n","epoch 1093 training_loss_per_epoch 0.5751715898513794\n","epoch 1094 training_loss_per_epoch 0.5751706957817078\n","epoch 1095 training_loss_per_epoch 0.5751698017120361\n","epoch 1096 training_loss_per_epoch 0.5751688480377197\n","epoch 1097 training_loss_per_epoch 0.5751679539680481\n","epoch 1098 training_loss_per_epoch 0.5751671195030212\n","epoch 1099 training_loss_per_epoch 0.5751661658287048\n","epoch 1100 training_loss_per_epoch 0.5751653909683228\n","epoch 1101 training_loss_per_epoch 0.5751644372940063\n","epoch 1102 training_loss_per_epoch 0.5751636028289795\n","epoch 1103 training_loss_per_epoch 0.5751627683639526\n","epoch 1104 training_loss_per_epoch 0.5751619338989258\n","epoch 1105 training_loss_per_epoch 0.5751610398292542\n","epoch 1106 training_loss_per_epoch 0.5751602053642273\n","epoch 1107 training_loss_per_epoch 0.5751593112945557\n","epoch 1108 training_loss_per_epoch 0.5751585364341736\n","epoch 1109 training_loss_per_epoch 0.5751577019691467\n","epoch 1110 training_loss_per_epoch 0.5751568675041199\n","epoch 1111 training_loss_per_epoch 0.575156033039093\n","epoch 1112 training_loss_per_epoch 0.5751552581787109\n","epoch 1113 training_loss_per_epoch 0.5751544833183289\n","epoch 1114 training_loss_per_epoch 0.5751537084579468\n","epoch 1115 training_loss_per_epoch 0.5751528739929199\n","epoch 1116 training_loss_per_epoch 0.5751520991325378\n","epoch 1117 training_loss_per_epoch 0.575151264667511\n","epoch 1118 training_loss_per_epoch 0.5751504898071289\n","epoch 1119 training_loss_per_epoch 0.5751497745513916\n","epoch 1120 training_loss_per_epoch 0.5751489996910095\n","epoch 1121 training_loss_per_epoch 0.5751482844352722\n","epoch 1122 training_loss_per_epoch 0.5751475095748901\n","epoch 1123 training_loss_per_epoch 0.5751467347145081\n","epoch 1124 training_loss_per_epoch 0.5751460194587708\n","epoch 1125 training_loss_per_epoch 0.5751453042030334\n","epoch 1126 training_loss_per_epoch 0.5751445889472961\n","epoch 1127 training_loss_per_epoch 0.5751438140869141\n","epoch 1128 training_loss_per_epoch 0.5751430988311768\n","epoch 1129 training_loss_per_epoch 0.5751423835754395\n","epoch 1130 training_loss_per_epoch 0.5751416683197021\n","epoch 1131 training_loss_per_epoch 0.5751409530639648\n","epoch 1132 training_loss_per_epoch 0.5751402378082275\n","epoch 1133 training_loss_per_epoch 0.5751395225524902\n","epoch 1134 training_loss_per_epoch 0.5751388072967529\n","epoch 1135 training_loss_per_epoch 0.5751381516456604\n","epoch 1136 training_loss_per_epoch 0.5751374959945679\n","epoch 1137 training_loss_per_epoch 0.5751368403434753\n","epoch 1138 training_loss_per_epoch 0.575136125087738\n","epoch 1139 training_loss_per_epoch 0.5751354694366455\n","epoch 1140 training_loss_per_epoch 0.5751348733901978\n","epoch 1141 training_loss_per_epoch 0.5751341581344604\n","epoch 1142 training_loss_per_epoch 0.5751335024833679\n","epoch 1143 training_loss_per_epoch 0.5751327872276306\n","epoch 1144 training_loss_per_epoch 0.5751321315765381\n","epoch 1145 training_loss_per_epoch 0.5751314759254456\n","epoch 1146 training_loss_per_epoch 0.5751308798789978\n","epoch 1147 training_loss_per_epoch 0.5751302242279053\n","epoch 1148 training_loss_per_epoch 0.575129508972168\n","epoch 1149 training_loss_per_epoch 0.5751289129257202\n","epoch 1150 training_loss_per_epoch 0.5751283168792725\n","epoch 1151 training_loss_per_epoch 0.5751276612281799\n","epoch 1152 training_loss_per_epoch 0.5751270055770874\n","epoch 1153 training_loss_per_epoch 0.5751263499259949\n","epoch 1154 training_loss_per_epoch 0.5751257538795471\n","epoch 1155 training_loss_per_epoch 0.5751251578330994\n","epoch 1156 training_loss_per_epoch 0.5751245021820068\n","epoch 1157 training_loss_per_epoch 0.5751238465309143\n","epoch 1158 training_loss_per_epoch 0.5751231908798218\n","epoch 1159 training_loss_per_epoch 0.5751226544380188\n","epoch 1160 training_loss_per_epoch 0.5751219987869263\n","epoch 1161 training_loss_per_epoch 0.5751213431358337\n","epoch 1162 training_loss_per_epoch 0.575120747089386\n","epoch 1163 training_loss_per_epoch 0.5751201510429382\n","epoch 1164 training_loss_per_epoch 0.5751195549964905\n","epoch 1165 training_loss_per_epoch 0.5751189589500427\n","epoch 1166 training_loss_per_epoch 0.5751183032989502\n","epoch 1167 training_loss_per_epoch 0.5751177072525024\n","epoch 1168 training_loss_per_epoch 0.5751171112060547\n","epoch 1169 training_loss_per_epoch 0.5751165151596069\n","epoch 1170 training_loss_per_epoch 0.5751159191131592\n","epoch 1171 training_loss_per_epoch 0.5751153230667114\n","epoch 1172 training_loss_per_epoch 0.5751147270202637\n","epoch 1173 training_loss_per_epoch 0.5751141309738159\n","epoch 1174 training_loss_per_epoch 0.5751135945320129\n","epoch 1175 training_loss_per_epoch 0.5751129388809204\n","epoch 1176 training_loss_per_epoch 0.5751124024391174\n","epoch 1177 training_loss_per_epoch 0.5751118063926697\n","epoch 1178 training_loss_per_epoch 0.5751112103462219\n","epoch 1179 training_loss_per_epoch 0.575110673904419\n","epoch 1180 training_loss_per_epoch 0.5751100778579712\n","epoch 1181 training_loss_per_epoch 0.5751094818115234\n","epoch 1182 training_loss_per_epoch 0.5751088857650757\n","epoch 1183 training_loss_per_epoch 0.5751082897186279\n","epoch 1184 training_loss_per_epoch 0.5751076936721802\n","epoch 1185 training_loss_per_epoch 0.575107216835022\n","epoch 1186 training_loss_per_epoch 0.5751066207885742\n","epoch 1187 training_loss_per_epoch 0.5751060843467712\n","epoch 1188 training_loss_per_epoch 0.5751054883003235\n","epoch 1189 training_loss_per_epoch 0.5751049518585205\n","epoch 1190 training_loss_per_epoch 0.5751044154167175\n","epoch 1191 training_loss_per_epoch 0.5751038193702698\n","epoch 1192 training_loss_per_epoch 0.5751032829284668\n","epoch 1193 training_loss_per_epoch 0.5751027464866638\n","epoch 1194 training_loss_per_epoch 0.5751022696495056\n","epoch 1195 training_loss_per_epoch 0.5751017332077026\n","epoch 1196 training_loss_per_epoch 0.5751011371612549\n","epoch 1197 training_loss_per_epoch 0.5751006603240967\n","epoch 1198 training_loss_per_epoch 0.5751001238822937\n","epoch 1199 training_loss_per_epoch 0.5750996470451355\n","epoch 1200 training_loss_per_epoch 0.5750991106033325\n","epoch 1201 training_loss_per_epoch 0.5750985741615295\n","epoch 1202 training_loss_per_epoch 0.5750980973243713\n","epoch 1203 training_loss_per_epoch 0.5750975608825684\n","epoch 1204 training_loss_per_epoch 0.5750970840454102\n","epoch 1205 training_loss_per_epoch 0.5750965476036072\n","epoch 1206 training_loss_per_epoch 0.575096070766449\n","epoch 1207 training_loss_per_epoch 0.5750955939292908\n","epoch 1208 training_loss_per_epoch 0.575094997882843\n","epoch 1209 training_loss_per_epoch 0.5750945210456848\n","epoch 1210 training_loss_per_epoch 0.5750939249992371\n","epoch 1211 training_loss_per_epoch 0.5750934481620789\n","epoch 1212 training_loss_per_epoch 0.5750929117202759\n","epoch 1213 training_loss_per_epoch 0.5750924348831177\n","epoch 1214 training_loss_per_epoch 0.5750918388366699\n","epoch 1215 training_loss_per_epoch 0.5750913619995117\n","epoch 1216 training_loss_per_epoch 0.5750908851623535\n","epoch 1217 training_loss_per_epoch 0.5750903487205505\n","epoch 1218 training_loss_per_epoch 0.5750898122787476\n","epoch 1219 training_loss_per_epoch 0.5750893354415894\n","epoch 1220 training_loss_per_epoch 0.5750888586044312\n","epoch 1221 training_loss_per_epoch 0.5750883221626282\n","epoch 1222 training_loss_per_epoch 0.57508784532547\n","epoch 1223 training_loss_per_epoch 0.575087308883667\n","epoch 1224 training_loss_per_epoch 0.5750868320465088\n","epoch 1225 training_loss_per_epoch 0.5750863552093506\n","epoch 1226 training_loss_per_epoch 0.5750858187675476\n","epoch 1227 training_loss_per_epoch 0.5750853419303894\n","epoch 1228 training_loss_per_epoch 0.5750848650932312\n","epoch 1229 training_loss_per_epoch 0.5750844478607178\n","epoch 1230 training_loss_per_epoch 0.5750839114189148\n","epoch 1231 training_loss_per_epoch 0.5750834345817566\n","epoch 1232 training_loss_per_epoch 0.5750829577445984\n","epoch 1233 training_loss_per_epoch 0.5750824213027954\n","epoch 1234 training_loss_per_epoch 0.5750819444656372\n","epoch 1235 training_loss_per_epoch 0.575081467628479\n","epoch 1236 training_loss_per_epoch 0.5750809907913208\n","epoch 1237 training_loss_per_epoch 0.5750805139541626\n","epoch 1238 training_loss_per_epoch 0.5750799775123596\n","epoch 1239 training_loss_per_epoch 0.5750795006752014\n","epoch 1240 training_loss_per_epoch 0.5750789642333984\n","epoch 1241 training_loss_per_epoch 0.5750784873962402\n","epoch 1242 training_loss_per_epoch 0.575078010559082\n","epoch 1243 training_loss_per_epoch 0.575077474117279\n","epoch 1244 training_loss_per_epoch 0.5750769972801208\n","epoch 1245 training_loss_per_epoch 0.5750765204429626\n","epoch 1246 training_loss_per_epoch 0.5750759840011597\n","epoch 1247 training_loss_per_epoch 0.5750754475593567\n","epoch 1248 training_loss_per_epoch 0.5750749111175537\n","epoch 1249 training_loss_per_epoch 0.575074315071106\n","epoch 1250 training_loss_per_epoch 0.575073778629303\n","epoch 1251 training_loss_per_epoch 0.5750732421875\n","epoch 1252 training_loss_per_epoch 0.575072705745697\n","epoch 1253 training_loss_per_epoch 0.575072169303894\n","epoch 1254 training_loss_per_epoch 0.5750716924667358\n","epoch 1255 training_loss_per_epoch 0.5750711560249329\n","epoch 1256 training_loss_per_epoch 0.5750707387924194\n","epoch 1257 training_loss_per_epoch 0.5750702023506165\n","epoch 1258 training_loss_per_epoch 0.575069785118103\n","epoch 1259 training_loss_per_epoch 0.5750693082809448\n","epoch 1260 training_loss_per_epoch 0.5750687718391418\n","epoch 1261 training_loss_per_epoch 0.5750683546066284\n","epoch 1262 training_loss_per_epoch 0.5750678777694702\n","epoch 1263 training_loss_per_epoch 0.575067400932312\n","epoch 1264 training_loss_per_epoch 0.5750669836997986\n","epoch 1265 training_loss_per_epoch 0.5750665068626404\n","epoch 1266 training_loss_per_epoch 0.5750659704208374\n","epoch 1267 training_loss_per_epoch 0.575065553188324\n","epoch 1268 training_loss_per_epoch 0.5750651359558105\n","epoch 1269 training_loss_per_epoch 0.5750646591186523\n","epoch 1270 training_loss_per_epoch 0.5750641822814941\n","epoch 1271 training_loss_per_epoch 0.5750637650489807\n","epoch 1272 training_loss_per_epoch 0.5750632882118225\n","epoch 1273 training_loss_per_epoch 0.5750628113746643\n","epoch 1274 training_loss_per_epoch 0.5750623941421509\n","epoch 1275 training_loss_per_epoch 0.5750619173049927\n","epoch 1276 training_loss_per_epoch 0.5750614404678345\n","epoch 1277 training_loss_per_epoch 0.575061023235321\n","epoch 1278 training_loss_per_epoch 0.5750606060028076\n","epoch 1279 training_loss_per_epoch 0.5750601291656494\n","epoch 1280 training_loss_per_epoch 0.575059711933136\n","epoch 1281 training_loss_per_epoch 0.5750592350959778\n","epoch 1282 training_loss_per_epoch 0.5750588178634644\n","epoch 1283 training_loss_per_epoch 0.5750584006309509\n","epoch 1284 training_loss_per_epoch 0.5750579833984375\n","epoch 1285 training_loss_per_epoch 0.5750575065612793\n","epoch 1286 training_loss_per_epoch 0.5750570893287659\n","epoch 1287 training_loss_per_epoch 0.5750566124916077\n","epoch 1288 training_loss_per_epoch 0.575056254863739\n","epoch 1289 training_loss_per_epoch 0.5750557780265808\n","epoch 1290 training_loss_per_epoch 0.5750553011894226\n","epoch 1291 training_loss_per_epoch 0.5750548839569092\n","epoch 1292 training_loss_per_epoch 0.5750544667243958\n","epoch 1293 training_loss_per_epoch 0.5750539898872375\n","epoch 1294 training_loss_per_epoch 0.5750535130500793\n","epoch 1295 training_loss_per_epoch 0.5750530958175659\n","epoch 1296 training_loss_per_epoch 0.5750526785850525\n","epoch 1297 training_loss_per_epoch 0.5750522613525391\n","epoch 1298 training_loss_per_epoch 0.5750517845153809\n","epoch 1299 training_loss_per_epoch 0.5750513672828674\n","epoch 1300 training_loss_per_epoch 0.575050950050354\n","epoch 1301 training_loss_per_epoch 0.5750504732131958\n","epoch 1302 training_loss_per_epoch 0.5750500559806824\n","epoch 1303 training_loss_per_epoch 0.5750495791435242\n","epoch 1304 training_loss_per_epoch 0.5750491619110107\n","epoch 1305 training_loss_per_epoch 0.5750487446784973\n","epoch 1306 training_loss_per_epoch 0.5750482678413391\n","epoch 1307 training_loss_per_epoch 0.5750479102134705\n","epoch 1308 training_loss_per_epoch 0.5750474333763123\n","epoch 1309 training_loss_per_epoch 0.5750470161437988\n","epoch 1310 training_loss_per_epoch 0.5750465989112854\n","epoch 1311 training_loss_per_epoch 0.5750461220741272\n","epoch 1312 training_loss_per_epoch 0.5750457644462585\n","epoch 1313 training_loss_per_epoch 0.5750452876091003\n","epoch 1314 training_loss_per_epoch 0.5750448703765869\n","epoch 1315 training_loss_per_epoch 0.5750444531440735\n","epoch 1316 training_loss_per_epoch 0.5750439763069153\n","epoch 1317 training_loss_per_epoch 0.5750435590744019\n","epoch 1318 training_loss_per_epoch 0.5750432014465332\n","epoch 1319 training_loss_per_epoch 0.5750427842140198\n","epoch 1320 training_loss_per_epoch 0.5750423669815063\n","epoch 1321 training_loss_per_epoch 0.5750419497489929\n","epoch 1322 training_loss_per_epoch 0.5750415325164795\n","epoch 1323 training_loss_per_epoch 0.5750410556793213\n","epoch 1324 training_loss_per_epoch 0.5750406980514526\n","epoch 1325 training_loss_per_epoch 0.5750402808189392\n","epoch 1326 training_loss_per_epoch 0.575039803981781\n","epoch 1327 training_loss_per_epoch 0.5750394463539124\n","epoch 1328 training_loss_per_epoch 0.5750390291213989\n","epoch 1329 training_loss_per_epoch 0.5750386118888855\n","epoch 1330 training_loss_per_epoch 0.5750381946563721\n","epoch 1331 training_loss_per_epoch 0.5750377774238586\n","epoch 1332 training_loss_per_epoch 0.57503741979599\n","epoch 1333 training_loss_per_epoch 0.5750370025634766\n","epoch 1334 training_loss_per_epoch 0.5750365853309631\n","epoch 1335 training_loss_per_epoch 0.5750362277030945\n","epoch 1336 training_loss_per_epoch 0.5750357508659363\n","epoch 1337 training_loss_per_epoch 0.5750353336334229\n","epoch 1338 training_loss_per_epoch 0.5750349760055542\n","epoch 1339 training_loss_per_epoch 0.5750345587730408\n","epoch 1340 training_loss_per_epoch 0.5750342011451721\n","epoch 1341 training_loss_per_epoch 0.5750337839126587\n","epoch 1342 training_loss_per_epoch 0.57503342628479\n","epoch 1343 training_loss_per_epoch 0.5750330090522766\n","epoch 1344 training_loss_per_epoch 0.575032651424408\n","epoch 1345 training_loss_per_epoch 0.5750322341918945\n","epoch 1346 training_loss_per_epoch 0.5750318765640259\n","epoch 1347 training_loss_per_epoch 0.5750314593315125\n","epoch 1348 training_loss_per_epoch 0.575031042098999\n","epoch 1349 training_loss_per_epoch 0.5750306844711304\n","epoch 1350 training_loss_per_epoch 0.5750303268432617\n","epoch 1351 training_loss_per_epoch 0.5750299096107483\n","epoch 1352 training_loss_per_epoch 0.5750295519828796\n","epoch 1353 training_loss_per_epoch 0.5750290751457214\n","epoch 1354 training_loss_per_epoch 0.5750287175178528\n","epoch 1355 training_loss_per_epoch 0.5750283598899841\n","epoch 1356 training_loss_per_epoch 0.5750280022621155\n","epoch 1357 training_loss_per_epoch 0.575027585029602\n","epoch 1358 training_loss_per_epoch 0.5750271677970886\n","epoch 1359 training_loss_per_epoch 0.57502681016922\n","epoch 1360 training_loss_per_epoch 0.5750264525413513\n","epoch 1361 training_loss_per_epoch 0.5750260353088379\n","epoch 1362 training_loss_per_epoch 0.5750256776809692\n","epoch 1363 training_loss_per_epoch 0.5750252604484558\n","epoch 1364 training_loss_per_epoch 0.5750249028205872\n","epoch 1365 training_loss_per_epoch 0.5750244855880737\n","epoch 1366 training_loss_per_epoch 0.5750241279602051\n","epoch 1367 training_loss_per_epoch 0.5750237107276917\n","epoch 1368 training_loss_per_epoch 0.575023353099823\n","epoch 1369 training_loss_per_epoch 0.5750229954719543\n","epoch 1370 training_loss_per_epoch 0.5750225782394409\n","epoch 1371 training_loss_per_epoch 0.5750222206115723\n","epoch 1372 training_loss_per_epoch 0.5750218033790588\n","epoch 1373 training_loss_per_epoch 0.5750214457511902\n","epoch 1374 training_loss_per_epoch 0.5750210285186768\n","epoch 1375 training_loss_per_epoch 0.5750206708908081\n","epoch 1376 training_loss_per_epoch 0.5750202536582947\n","epoch 1377 training_loss_per_epoch 0.575019896030426\n","epoch 1378 training_loss_per_epoch 0.5750194787979126\n","epoch 1379 training_loss_per_epoch 0.575019121170044\n","epoch 1380 training_loss_per_epoch 0.5750187039375305\n","epoch 1381 training_loss_per_epoch 0.5750182867050171\n","epoch 1382 training_loss_per_epoch 0.5750179290771484\n","epoch 1383 training_loss_per_epoch 0.5750174522399902\n","epoch 1384 training_loss_per_epoch 0.5750170946121216\n","epoch 1385 training_loss_per_epoch 0.5750166773796082\n","epoch 1386 training_loss_per_epoch 0.5750163197517395\n","epoch 1387 training_loss_per_epoch 0.5750159025192261\n","epoch 1388 training_loss_per_epoch 0.5750155448913574\n","epoch 1389 training_loss_per_epoch 0.575015127658844\n","epoch 1390 training_loss_per_epoch 0.5750147700309753\n","epoch 1391 training_loss_per_epoch 0.5750143527984619\n","epoch 1392 training_loss_per_epoch 0.5750139951705933\n","epoch 1393 training_loss_per_epoch 0.5750136375427246\n","epoch 1394 training_loss_per_epoch 0.575013279914856\n","epoch 1395 training_loss_per_epoch 0.5750129222869873\n","epoch 1396 training_loss_per_epoch 0.5750125646591187\n","epoch 1397 training_loss_per_epoch 0.57501220703125\n","epoch 1398 training_loss_per_epoch 0.5750118494033813\n","epoch 1399 training_loss_per_epoch 0.5750114917755127\n","epoch 1400 training_loss_per_epoch 0.575011134147644\n","epoch 1401 training_loss_per_epoch 0.5750107765197754\n","epoch 1402 training_loss_per_epoch 0.5750104188919067\n","epoch 1403 training_loss_per_epoch 0.5750100612640381\n","epoch 1404 training_loss_per_epoch 0.5750097036361694\n","epoch 1405 training_loss_per_epoch 0.5750093460083008\n","epoch 1406 training_loss_per_epoch 0.5750089883804321\n","epoch 1407 training_loss_per_epoch 0.5750086307525635\n","epoch 1408 training_loss_per_epoch 0.5750082731246948\n","epoch 1409 training_loss_per_epoch 0.575007975101471\n","epoch 1410 training_loss_per_epoch 0.5750076174736023\n","epoch 1411 training_loss_per_epoch 0.5750072598457336\n","epoch 1412 training_loss_per_epoch 0.575006902217865\n","epoch 1413 training_loss_per_epoch 0.5750065445899963\n","epoch 1414 training_loss_per_epoch 0.5750062465667725\n","epoch 1415 training_loss_per_epoch 0.5750058889389038\n","epoch 1416 training_loss_per_epoch 0.5750055313110352\n","epoch 1417 training_loss_per_epoch 0.5750052332878113\n","epoch 1418 training_loss_per_epoch 0.5750048160552979\n","epoch 1419 training_loss_per_epoch 0.5750044584274292\n","epoch 1420 training_loss_per_epoch 0.5750042200088501\n","epoch 1421 training_loss_per_epoch 0.5750038027763367\n","epoch 1422 training_loss_per_epoch 0.575003445148468\n","epoch 1423 training_loss_per_epoch 0.5750030875205994\n","epoch 1424 training_loss_per_epoch 0.5750027894973755\n","epoch 1425 training_loss_per_epoch 0.5750024318695068\n","epoch 1426 training_loss_per_epoch 0.5750020742416382\n","epoch 1427 training_loss_per_epoch 0.5750017166137695\n","epoch 1428 training_loss_per_epoch 0.5750012993812561\n","epoch 1429 training_loss_per_epoch 0.5750009417533875\n","epoch 1430 training_loss_per_epoch 0.5750006437301636\n","epoch 1431 training_loss_per_epoch 0.5750002264976501\n","epoch 1432 training_loss_per_epoch 0.5749998688697815\n","epoch 1433 training_loss_per_epoch 0.5749995112419128\n","epoch 1434 training_loss_per_epoch 0.5749991536140442\n","epoch 1435 training_loss_per_epoch 0.5749987959861755\n","epoch 1436 training_loss_per_epoch 0.5749984383583069\n","epoch 1437 training_loss_per_epoch 0.5749980211257935\n","epoch 1438 training_loss_per_epoch 0.5749976634979248\n","epoch 1439 training_loss_per_epoch 0.5749973058700562\n","epoch 1440 training_loss_per_epoch 0.5749969482421875\n","epoch 1441 training_loss_per_epoch 0.5749965906143188\n","epoch 1442 training_loss_per_epoch 0.5749962329864502\n","epoch 1443 training_loss_per_epoch 0.5749958753585815\n","epoch 1444 training_loss_per_epoch 0.5749955177307129\n","epoch 1445 training_loss_per_epoch 0.5749951004981995\n","epoch 1446 training_loss_per_epoch 0.5749947428703308\n","epoch 1447 training_loss_per_epoch 0.5749943852424622\n","epoch 1448 training_loss_per_epoch 0.5749940276145935\n","epoch 1449 training_loss_per_epoch 0.5749936699867249\n","epoch 1450 training_loss_per_epoch 0.5749933123588562\n","epoch 1451 training_loss_per_epoch 0.5749928951263428\n","epoch 1452 training_loss_per_epoch 0.5749925374984741\n","epoch 1453 training_loss_per_epoch 0.5749921202659607\n","epoch 1454 training_loss_per_epoch 0.5749918222427368\n","epoch 1455 training_loss_per_epoch 0.5749914050102234\n","epoch 1456 training_loss_per_epoch 0.5749910473823547\n","epoch 1457 training_loss_per_epoch 0.5749906897544861\n","epoch 1458 training_loss_per_epoch 0.5749903321266174\n","epoch 1459 training_loss_per_epoch 0.574989914894104\n","epoch 1460 training_loss_per_epoch 0.5749895572662354\n","epoch 1461 training_loss_per_epoch 0.5749891400337219\n","epoch 1462 training_loss_per_epoch 0.5749887824058533\n","epoch 1463 training_loss_per_epoch 0.5749884247779846\n","epoch 1464 training_loss_per_epoch 0.5749880075454712\n","epoch 1465 training_loss_per_epoch 0.5749876499176025\n","epoch 1466 training_loss_per_epoch 0.5749872922897339\n","epoch 1467 training_loss_per_epoch 0.5749869346618652\n","epoch 1468 training_loss_per_epoch 0.5749866366386414\n","epoch 1469 training_loss_per_epoch 0.5749862194061279\n","epoch 1470 training_loss_per_epoch 0.5749858617782593\n","epoch 1471 training_loss_per_epoch 0.5749855637550354\n","epoch 1472 training_loss_per_epoch 0.5749852061271667\n","epoch 1473 training_loss_per_epoch 0.5749847888946533\n","epoch 1474 training_loss_per_epoch 0.5749844312667847\n","epoch 1475 training_loss_per_epoch 0.5749841332435608\n","epoch 1476 training_loss_per_epoch 0.5749837756156921\n","epoch 1477 training_loss_per_epoch 0.5749833583831787\n","epoch 1478 training_loss_per_epoch 0.5749830007553101\n","epoch 1479 training_loss_per_epoch 0.5749826431274414\n","epoch 1480 training_loss_per_epoch 0.5749822854995728\n","epoch 1481 training_loss_per_epoch 0.5749819278717041\n","epoch 1482 training_loss_per_epoch 0.5749815106391907\n","epoch 1483 training_loss_per_epoch 0.5749810934066772\n","epoch 1484 training_loss_per_epoch 0.5749807357788086\n","epoch 1485 training_loss_per_epoch 0.5749803185462952\n","epoch 1486 training_loss_per_epoch 0.5749799609184265\n","epoch 1487 training_loss_per_epoch 0.5749796032905579\n","epoch 1488 training_loss_per_epoch 0.5749791860580444\n","epoch 1489 training_loss_per_epoch 0.574978768825531\n","epoch 1490 training_loss_per_epoch 0.5749784708023071\n","epoch 1491 training_loss_per_epoch 0.5749781131744385\n","epoch 1492 training_loss_per_epoch 0.5749778151512146\n","epoch 1493 training_loss_per_epoch 0.5749773979187012\n","epoch 1494 training_loss_per_epoch 0.5749770402908325\n","epoch 1495 training_loss_per_epoch 0.5749767422676086\n","epoch 1496 training_loss_per_epoch 0.57497638463974\n","epoch 1497 training_loss_per_epoch 0.5749760270118713\n","epoch 1498 training_loss_per_epoch 0.5749756693840027\n","epoch 1499 training_loss_per_epoch 0.574975311756134\n","epoch 1500 training_loss_per_epoch 0.5749750137329102\n","epoch 1501 training_loss_per_epoch 0.5749746561050415\n","epoch 1502 training_loss_per_epoch 0.5749743580818176\n","epoch 1503 training_loss_per_epoch 0.5749740600585938\n","epoch 1504 training_loss_per_epoch 0.5749737620353699\n","epoch 1505 training_loss_per_epoch 0.5749734044075012\n","epoch 1506 training_loss_per_epoch 0.5749730467796326\n","epoch 1507 training_loss_per_epoch 0.5749727487564087\n","epoch 1508 training_loss_per_epoch 0.5749724507331848\n","epoch 1509 training_loss_per_epoch 0.5749720931053162\n","epoch 1510 training_loss_per_epoch 0.5749717950820923\n","epoch 1511 training_loss_per_epoch 0.5749714970588684\n","epoch 1512 training_loss_per_epoch 0.5749712586402893\n","epoch 1513 training_loss_per_epoch 0.5749709606170654\n","epoch 1514 training_loss_per_epoch 0.5749706029891968\n","epoch 1515 training_loss_per_epoch 0.5749703049659729\n","epoch 1516 training_loss_per_epoch 0.5749700665473938\n","epoch 1517 training_loss_per_epoch 0.5749697089195251\n","epoch 1518 training_loss_per_epoch 0.5749694108963013\n","epoch 1519 training_loss_per_epoch 0.5749691128730774\n","epoch 1520 training_loss_per_epoch 0.5749688148498535\n","epoch 1521 training_loss_per_epoch 0.5749684572219849\n","epoch 1522 training_loss_per_epoch 0.5749682188034058\n","epoch 1523 training_loss_per_epoch 0.5749678611755371\n","epoch 1524 training_loss_per_epoch 0.5749675631523132\n","epoch 1525 training_loss_per_epoch 0.5749672651290894\n","epoch 1526 training_loss_per_epoch 0.5749669075012207\n","epoch 1527 training_loss_per_epoch 0.5749666690826416\n","epoch 1528 training_loss_per_epoch 0.574966311454773\n","epoch 1529 training_loss_per_epoch 0.5749660134315491\n","epoch 1530 training_loss_per_epoch 0.57496577501297\n","epoch 1531 training_loss_per_epoch 0.5749654769897461\n","epoch 1532 training_loss_per_epoch 0.574965238571167\n","epoch 1533 training_loss_per_epoch 0.5749649405479431\n","epoch 1534 training_loss_per_epoch 0.5749646425247192\n","epoch 1535 training_loss_per_epoch 0.5749643445014954\n","epoch 1536 training_loss_per_epoch 0.5749640464782715\n","epoch 1537 training_loss_per_epoch 0.5749637484550476\n","epoch 1538 training_loss_per_epoch 0.5749634504318237\n","epoch 1539 training_loss_per_epoch 0.5749630928039551\n","epoch 1540 training_loss_per_epoch 0.574962854385376\n","epoch 1541 training_loss_per_epoch 0.5749625563621521\n","epoch 1542 training_loss_per_epoch 0.574962317943573\n","epoch 1543 training_loss_per_epoch 0.5749620199203491\n","epoch 1544 training_loss_per_epoch 0.5749617218971252\n","epoch 1545 training_loss_per_epoch 0.5749614834785461\n","epoch 1546 training_loss_per_epoch 0.5749611854553223\n","epoch 1547 training_loss_per_epoch 0.5749608874320984\n","epoch 1548 training_loss_per_epoch 0.5749605894088745\n","epoch 1549 training_loss_per_epoch 0.5749603509902954\n","epoch 1550 training_loss_per_epoch 0.5749601125717163\n","epoch 1551 training_loss_per_epoch 0.5749598145484924\n","epoch 1552 training_loss_per_epoch 0.5749595165252686\n","epoch 1553 training_loss_per_epoch 0.5749592781066895\n","epoch 1554 training_loss_per_epoch 0.5749589800834656\n","epoch 1555 training_loss_per_epoch 0.5749586820602417\n","epoch 1556 training_loss_per_epoch 0.5749583840370178\n","epoch 1557 training_loss_per_epoch 0.574958086013794\n","epoch 1558 training_loss_per_epoch 0.5749577879905701\n","epoch 1559 training_loss_per_epoch 0.5749574899673462\n","epoch 1560 training_loss_per_epoch 0.5749572515487671\n","epoch 1561 training_loss_per_epoch 0.5749569535255432\n","epoch 1562 training_loss_per_epoch 0.5749566555023193\n","epoch 1563 training_loss_per_epoch 0.5749563574790955\n","epoch 1564 training_loss_per_epoch 0.5749560594558716\n","epoch 1565 training_loss_per_epoch 0.5749557614326477\n","epoch 1566 training_loss_per_epoch 0.5749554634094238\n","epoch 1567 training_loss_per_epoch 0.5749552249908447\n","epoch 1568 training_loss_per_epoch 0.5749549269676208\n","epoch 1569 training_loss_per_epoch 0.574954628944397\n","epoch 1570 training_loss_per_epoch 0.5749543309211731\n","epoch 1571 training_loss_per_epoch 0.5749540328979492\n","epoch 1572 training_loss_per_epoch 0.5749537348747253\n","epoch 1573 training_loss_per_epoch 0.5749534964561462\n","epoch 1574 training_loss_per_epoch 0.5749531984329224\n","epoch 1575 training_loss_per_epoch 0.5749529600143433\n","epoch 1576 training_loss_per_epoch 0.5749527215957642\n","epoch 1577 training_loss_per_epoch 0.5749524235725403\n","epoch 1578 training_loss_per_epoch 0.5749521851539612\n","epoch 1579 training_loss_per_epoch 0.5749518871307373\n","epoch 1580 training_loss_per_epoch 0.5749515891075134\n","epoch 1581 training_loss_per_epoch 0.5749513506889343\n","epoch 1582 training_loss_per_epoch 0.5749511122703552\n","epoch 1583 training_loss_per_epoch 0.5749508142471313\n","epoch 1584 training_loss_per_epoch 0.5749505758285522\n","epoch 1585 training_loss_per_epoch 0.5749502778053284\n","epoch 1586 training_loss_per_epoch 0.5749500393867493\n","epoch 1587 training_loss_per_epoch 0.5749497413635254\n","epoch 1588 training_loss_per_epoch 0.5749495029449463\n","epoch 1589 training_loss_per_epoch 0.5749492645263672\n","epoch 1590 training_loss_per_epoch 0.5749489665031433\n","epoch 1591 training_loss_per_epoch 0.5749487280845642\n","epoch 1592 training_loss_per_epoch 0.5749484896659851\n","epoch 1593 training_loss_per_epoch 0.5749481916427612\n","epoch 1594 training_loss_per_epoch 0.5749479532241821\n","epoch 1595 training_loss_per_epoch 0.574947714805603\n","epoch 1596 training_loss_per_epoch 0.5749474763870239\n","epoch 1597 training_loss_per_epoch 0.5749471783638\n","epoch 1598 training_loss_per_epoch 0.574946939945221\n","epoch 1599 training_loss_per_epoch 0.5749467015266418\n","epoch 1600 training_loss_per_epoch 0.574946403503418\n","epoch 1601 training_loss_per_epoch 0.5749462246894836\n","epoch 1602 training_loss_per_epoch 0.5749459266662598\n","epoch 1603 training_loss_per_epoch 0.5749456286430359\n","epoch 1604 training_loss_per_epoch 0.5749454498291016\n","epoch 1605 training_loss_per_epoch 0.5749451518058777\n","epoch 1606 training_loss_per_epoch 0.5749449729919434\n","epoch 1607 training_loss_per_epoch 0.5749446749687195\n","epoch 1608 training_loss_per_epoch 0.5749444365501404\n","epoch 1609 training_loss_per_epoch 0.5749441981315613\n","epoch 1610 training_loss_per_epoch 0.5749439597129822\n","epoch 1611 training_loss_per_epoch 0.5749437212944031\n","epoch 1612 training_loss_per_epoch 0.5749434232711792\n","epoch 1613 training_loss_per_epoch 0.5749431848526001\n","epoch 1614 training_loss_per_epoch 0.5749430060386658\n","epoch 1615 training_loss_per_epoch 0.5749427080154419\n","epoch 1616 training_loss_per_epoch 0.5749424695968628\n","epoch 1617 training_loss_per_epoch 0.5749422311782837\n","epoch 1618 training_loss_per_epoch 0.5749420523643494\n","epoch 1619 training_loss_per_epoch 0.5749418139457703\n","epoch 1620 training_loss_per_epoch 0.5749415755271912\n","epoch 1621 training_loss_per_epoch 0.5749413371086121\n","epoch 1622 training_loss_per_epoch 0.574941098690033\n","epoch 1623 training_loss_per_epoch 0.5749409198760986\n","epoch 1624 training_loss_per_epoch 0.5749406814575195\n","epoch 1625 training_loss_per_epoch 0.5749404430389404\n","epoch 1626 training_loss_per_epoch 0.5749402046203613\n","epoch 1627 training_loss_per_epoch 0.574940025806427\n","epoch 1628 training_loss_per_epoch 0.5749397873878479\n","epoch 1629 training_loss_per_epoch 0.5749395489692688\n","epoch 1630 training_loss_per_epoch 0.5749393105506897\n","epoch 1631 training_loss_per_epoch 0.5749390721321106\n","epoch 1632 training_loss_per_epoch 0.5749388337135315\n","epoch 1633 training_loss_per_epoch 0.5749386548995972\n","epoch 1634 training_loss_per_epoch 0.5749384164810181\n","epoch 1635 training_loss_per_epoch 0.574938178062439\n","epoch 1636 training_loss_per_epoch 0.5749379992485046\n","epoch 1637 training_loss_per_epoch 0.5749377012252808\n","epoch 1638 training_loss_per_epoch 0.5749374628067017\n","epoch 1639 training_loss_per_epoch 0.5749372243881226\n","epoch 1640 training_loss_per_epoch 0.5749370455741882\n","epoch 1641 training_loss_per_epoch 0.5749368071556091\n","epoch 1642 training_loss_per_epoch 0.5749366283416748\n","epoch 1643 training_loss_per_epoch 0.5749364495277405\n","epoch 1644 training_loss_per_epoch 0.5749361515045166\n","epoch 1645 training_loss_per_epoch 0.5749359726905823\n","epoch 1646 training_loss_per_epoch 0.5749357342720032\n","epoch 1647 training_loss_per_epoch 0.5749355554580688\n","epoch 1648 training_loss_per_epoch 0.5749353170394897\n","epoch 1649 training_loss_per_epoch 0.5749350786209106\n","epoch 1650 training_loss_per_epoch 0.5749348998069763\n","epoch 1651 training_loss_per_epoch 0.5749346613883972\n","epoch 1652 training_loss_per_epoch 0.5749344825744629\n","epoch 1653 training_loss_per_epoch 0.5749342441558838\n","epoch 1654 training_loss_per_epoch 0.5749340057373047\n","epoch 1655 training_loss_per_epoch 0.5749338269233704\n","epoch 1656 training_loss_per_epoch 0.5749335289001465\n","epoch 1657 training_loss_per_epoch 0.5749333500862122\n","epoch 1658 training_loss_per_epoch 0.5749331712722778\n","epoch 1659 training_loss_per_epoch 0.5749329328536987\n","epoch 1660 training_loss_per_epoch 0.5749327540397644\n","epoch 1661 training_loss_per_epoch 0.5749325156211853\n","epoch 1662 training_loss_per_epoch 0.5749322772026062\n","epoch 1663 training_loss_per_epoch 0.5749320983886719\n","epoch 1664 training_loss_per_epoch 0.5749318599700928\n","epoch 1665 training_loss_per_epoch 0.5749316215515137\n","epoch 1666 training_loss_per_epoch 0.5749313831329346\n","epoch 1667 training_loss_per_epoch 0.5749311447143555\n","epoch 1668 training_loss_per_epoch 0.5749309659004211\n","epoch 1669 training_loss_per_epoch 0.574930727481842\n","epoch 1670 training_loss_per_epoch 0.5749304890632629\n","epoch 1671 training_loss_per_epoch 0.5749303102493286\n","epoch 1672 training_loss_per_epoch 0.5749300122261047\n","epoch 1673 training_loss_per_epoch 0.5749298334121704\n","epoch 1674 training_loss_per_epoch 0.5749295949935913\n","epoch 1675 training_loss_per_epoch 0.574929416179657\n","epoch 1676 training_loss_per_epoch 0.5749291777610779\n","epoch 1677 training_loss_per_epoch 0.5749289989471436\n","epoch 1678 training_loss_per_epoch 0.5749287605285645\n","epoch 1679 training_loss_per_epoch 0.5749285221099854\n","epoch 1680 training_loss_per_epoch 0.574928343296051\n","epoch 1681 training_loss_per_epoch 0.5749281048774719\n","epoch 1682 training_loss_per_epoch 0.5749279260635376\n","epoch 1683 training_loss_per_epoch 0.5749276876449585\n","epoch 1684 training_loss_per_epoch 0.5749275088310242\n","epoch 1685 training_loss_per_epoch 0.5749272704124451\n","epoch 1686 training_loss_per_epoch 0.574927031993866\n","epoch 1687 training_loss_per_epoch 0.5749268531799316\n","epoch 1688 training_loss_per_epoch 0.5749266743659973\n","epoch 1689 training_loss_per_epoch 0.5749264359474182\n","epoch 1690 training_loss_per_epoch 0.5749262571334839\n","epoch 1691 training_loss_per_epoch 0.5749260187149048\n","epoch 1692 training_loss_per_epoch 0.5749257802963257\n","epoch 1693 training_loss_per_epoch 0.5749256014823914\n","epoch 1694 training_loss_per_epoch 0.574925422668457\n","epoch 1695 training_loss_per_epoch 0.5749252438545227\n","epoch 1696 training_loss_per_epoch 0.5749249458312988\n","epoch 1697 training_loss_per_epoch 0.5749247670173645\n","epoch 1698 training_loss_per_epoch 0.5749245882034302\n","epoch 1699 training_loss_per_epoch 0.5749243497848511\n","epoch 1700 training_loss_per_epoch 0.5749241709709167\n","epoch 1701 training_loss_per_epoch 0.5749239921569824\n","epoch 1702 training_loss_per_epoch 0.5749238133430481\n","epoch 1703 training_loss_per_epoch 0.574923574924469\n","epoch 1704 training_loss_per_epoch 0.5749233961105347\n","epoch 1705 training_loss_per_epoch 0.5749232172966003\n","epoch 1706 training_loss_per_epoch 0.574923038482666\n","epoch 1707 training_loss_per_epoch 0.5749228000640869\n","epoch 1708 training_loss_per_epoch 0.5749225616455078\n","epoch 1709 training_loss_per_epoch 0.5749224424362183\n","epoch 1710 training_loss_per_epoch 0.5749222040176392\n","epoch 1711 training_loss_per_epoch 0.5749220252037048\n","epoch 1712 training_loss_per_epoch 0.5749218463897705\n","epoch 1713 training_loss_per_epoch 0.5749216079711914\n","epoch 1714 training_loss_per_epoch 0.5749214291572571\n","epoch 1715 training_loss_per_epoch 0.5749212503433228\n","epoch 1716 training_loss_per_epoch 0.5749210119247437\n","epoch 1717 training_loss_per_epoch 0.5749208927154541\n","epoch 1718 training_loss_per_epoch 0.574920654296875\n","epoch 1719 training_loss_per_epoch 0.5749204754829407\n","epoch 1720 training_loss_per_epoch 0.5749202966690063\n","epoch 1721 training_loss_per_epoch 0.574920117855072\n","epoch 1722 training_loss_per_epoch 0.5749198794364929\n","epoch 1723 training_loss_per_epoch 0.5749197006225586\n","epoch 1724 training_loss_per_epoch 0.5749195218086243\n","epoch 1725 training_loss_per_epoch 0.5749193429946899\n","epoch 1726 training_loss_per_epoch 0.5749191045761108\n","epoch 1727 training_loss_per_epoch 0.5749189257621765\n","epoch 1728 training_loss_per_epoch 0.5749187469482422\n","epoch 1729 training_loss_per_epoch 0.5749185681343079\n","epoch 1730 training_loss_per_epoch 0.5749183297157288\n","epoch 1731 training_loss_per_epoch 0.5749181509017944\n","epoch 1732 training_loss_per_epoch 0.5749179720878601\n","epoch 1733 training_loss_per_epoch 0.5749177932739258\n","epoch 1734 training_loss_per_epoch 0.5749176144599915\n","epoch 1735 training_loss_per_epoch 0.5749173760414124\n","epoch 1736 training_loss_per_epoch 0.574917197227478\n","epoch 1737 training_loss_per_epoch 0.5749170184135437\n","epoch 1738 training_loss_per_epoch 0.5749168395996094\n","epoch 1739 training_loss_per_epoch 0.574916660785675\n","epoch 1740 training_loss_per_epoch 0.574916422367096\n","epoch 1741 training_loss_per_epoch 0.5749162435531616\n","epoch 1742 training_loss_per_epoch 0.5749160647392273\n","epoch 1743 training_loss_per_epoch 0.574915885925293\n","epoch 1744 training_loss_per_epoch 0.5749157667160034\n","epoch 1745 training_loss_per_epoch 0.5749155282974243\n","epoch 1746 training_loss_per_epoch 0.57491534948349\n","epoch 1747 training_loss_per_epoch 0.5749151706695557\n","epoch 1748 training_loss_per_epoch 0.5749149918556213\n","epoch 1749 training_loss_per_epoch 0.5749147534370422\n","epoch 1750 training_loss_per_epoch 0.5749145746231079\n","epoch 1751 training_loss_per_epoch 0.5749143958091736\n","epoch 1752 training_loss_per_epoch 0.5749142169952393\n","epoch 1753 training_loss_per_epoch 0.5749140381813049\n","epoch 1754 training_loss_per_epoch 0.5749137997627258\n","epoch 1755 training_loss_per_epoch 0.5749136209487915\n","epoch 1756 training_loss_per_epoch 0.5749134421348572\n","epoch 1757 training_loss_per_epoch 0.5749132633209229\n","epoch 1758 training_loss_per_epoch 0.5749130249023438\n","epoch 1759 training_loss_per_epoch 0.5749128460884094\n","epoch 1760 training_loss_per_epoch 0.5749126672744751\n","epoch 1761 training_loss_per_epoch 0.574912428855896\n","epoch 1762 training_loss_per_epoch 0.5749121904373169\n","epoch 1763 training_loss_per_epoch 0.5749119520187378\n","epoch 1764 training_loss_per_epoch 0.5749116539955139\n","epoch 1765 training_loss_per_epoch 0.5749112963676453\n","epoch 1766 training_loss_per_epoch 0.5749109983444214\n","epoch 1767 training_loss_per_epoch 0.5749107003211975\n","epoch 1768 training_loss_per_epoch 0.5749104022979736\n","epoch 1769 training_loss_per_epoch 0.5749101638793945\n","epoch 1770 training_loss_per_epoch 0.5749098062515259\n","epoch 1771 training_loss_per_epoch 0.5749093890190125\n","epoch 1772 training_loss_per_epoch 0.5749090909957886\n","epoch 1773 training_loss_per_epoch 0.5749087929725647\n","epoch 1774 training_loss_per_epoch 0.5749084949493408\n","epoch 1775 training_loss_per_epoch 0.5749081969261169\n","epoch 1776 training_loss_per_epoch 0.5749080181121826\n","epoch 1777 training_loss_per_epoch 0.5749077796936035\n","epoch 1778 training_loss_per_epoch 0.5749075412750244\n","epoch 1779 training_loss_per_epoch 0.5749073624610901\n","epoch 1780 training_loss_per_epoch 0.574907124042511\n","epoch 1781 training_loss_per_epoch 0.5749069452285767\n","epoch 1782 training_loss_per_epoch 0.5749067664146423\n","epoch 1783 training_loss_per_epoch 0.574906587600708\n","epoch 1784 training_loss_per_epoch 0.5749062895774841\n","epoch 1785 training_loss_per_epoch 0.5749061107635498\n","epoch 1786 training_loss_per_epoch 0.5749058723449707\n","epoch 1787 training_loss_per_epoch 0.5749056935310364\n","epoch 1788 training_loss_per_epoch 0.5749053955078125\n","epoch 1789 training_loss_per_epoch 0.5749051570892334\n","epoch 1790 training_loss_per_epoch 0.5749049186706543\n","epoch 1791 training_loss_per_epoch 0.5749046206474304\n","epoch 1792 training_loss_per_epoch 0.5749043226242065\n","epoch 1793 training_loss_per_epoch 0.5749040842056274\n","epoch 1794 training_loss_per_epoch 0.5749037861824036\n","epoch 1795 training_loss_per_epoch 0.5749034881591797\n","epoch 1796 training_loss_per_epoch 0.5749032497406006\n","epoch 1797 training_loss_per_epoch 0.5749029517173767\n","epoch 1798 training_loss_per_epoch 0.5749027132987976\n","epoch 1799 training_loss_per_epoch 0.5749024152755737\n","epoch 1800 training_loss_per_epoch 0.5749021172523499\n","epoch 1801 training_loss_per_epoch 0.5749018788337708\n","epoch 1802 training_loss_per_epoch 0.5749015808105469\n","epoch 1803 training_loss_per_epoch 0.5749012231826782\n","epoch 1804 training_loss_per_epoch 0.5749009251594543\n","epoch 1805 training_loss_per_epoch 0.5749006271362305\n","epoch 1806 training_loss_per_epoch 0.5749002695083618\n","epoch 1807 training_loss_per_epoch 0.5748999714851379\n","epoch 1808 training_loss_per_epoch 0.5748996734619141\n","epoch 1809 training_loss_per_epoch 0.5748993754386902\n","epoch 1810 training_loss_per_epoch 0.5748990774154663\n","epoch 1811 training_loss_per_epoch 0.5748987793922424\n","epoch 1812 training_loss_per_epoch 0.5748984813690186\n","epoch 1813 training_loss_per_epoch 0.5748982429504395\n","epoch 1814 training_loss_per_epoch 0.5748978853225708\n","epoch 1815 training_loss_per_epoch 0.5748976469039917\n","epoch 1816 training_loss_per_epoch 0.574897289276123\n","epoch 1817 training_loss_per_epoch 0.5748969912528992\n","epoch 1818 training_loss_per_epoch 0.5748966932296753\n","epoch 1819 training_loss_per_epoch 0.5748963356018066\n","epoch 1820 training_loss_per_epoch 0.5748960971832275\n","epoch 1821 training_loss_per_epoch 0.5748957395553589\n","epoch 1822 training_loss_per_epoch 0.574895441532135\n","epoch 1823 training_loss_per_epoch 0.5748950839042664\n","epoch 1824 training_loss_per_epoch 0.5748946070671082\n","epoch 1825 training_loss_per_epoch 0.57489413022995\n","epoch 1826 training_loss_per_epoch 0.574893593788147\n","epoch 1827 training_loss_per_epoch 0.5748929977416992\n","epoch 1828 training_loss_per_epoch 0.5748923420906067\n","epoch 1829 training_loss_per_epoch 0.5748917460441589\n","epoch 1830 training_loss_per_epoch 0.5748911499977112\n","epoch 1831 training_loss_per_epoch 0.5748906135559082\n","epoch 1832 training_loss_per_epoch 0.5748900771141052\n","epoch 1833 training_loss_per_epoch 0.5748895406723022\n","epoch 1834 training_loss_per_epoch 0.5748889446258545\n","epoch 1835 training_loss_per_epoch 0.5748884081840515\n","epoch 1836 training_loss_per_epoch 0.5748876929283142\n","epoch 1837 training_loss_per_epoch 0.5748869776725769\n","epoch 1838 training_loss_per_epoch 0.57488614320755\n","epoch 1839 training_loss_per_epoch 0.5748853087425232\n","epoch 1840 training_loss_per_epoch 0.5748846530914307\n","epoch 1841 training_loss_per_epoch 0.5748841166496277\n","epoch 1842 training_loss_per_epoch 0.5748835206031799\n","epoch 1843 training_loss_per_epoch 0.5748828649520874\n","epoch 1844 training_loss_per_epoch 0.5748823881149292\n","epoch 1845 training_loss_per_epoch 0.5748820304870605\n","epoch 1846 training_loss_per_epoch 0.5748817324638367\n","epoch 1847 training_loss_per_epoch 0.5748814344406128\n","epoch 1848 training_loss_per_epoch 0.5748811364173889\n","epoch 1849 training_loss_per_epoch 0.5748807787895203\n","epoch 1850 training_loss_per_epoch 0.5748803615570068\n","epoch 1851 training_loss_per_epoch 0.5748800039291382\n","epoch 1852 training_loss_per_epoch 0.5748797059059143\n","epoch 1853 training_loss_per_epoch 0.5748793482780457\n","epoch 1854 training_loss_per_epoch 0.574878990650177\n","epoch 1855 training_loss_per_epoch 0.5748786330223083\n","epoch 1856 training_loss_per_epoch 0.5748783349990845\n","epoch 1857 training_loss_per_epoch 0.5748779773712158\n","epoch 1858 training_loss_per_epoch 0.5748777389526367\n","epoch 1859 training_loss_per_epoch 0.5748773813247681\n","epoch 1860 training_loss_per_epoch 0.574877142906189\n","epoch 1861 training_loss_per_epoch 0.5748768448829651\n","epoch 1862 training_loss_per_epoch 0.574876606464386\n","epoch 1863 training_loss_per_epoch 0.5748762488365173\n","epoch 1864 training_loss_per_epoch 0.5748759508132935\n","epoch 1865 training_loss_per_epoch 0.5748756527900696\n","epoch 1866 training_loss_per_epoch 0.5748753547668457\n","epoch 1867 training_loss_per_epoch 0.574874997138977\n","epoch 1868 training_loss_per_epoch 0.5748746991157532\n","epoch 1869 training_loss_per_epoch 0.5748744010925293\n","epoch 1870 training_loss_per_epoch 0.5748741626739502\n","epoch 1871 training_loss_per_epoch 0.5748738050460815\n","epoch 1872 training_loss_per_epoch 0.5748735070228577\n","epoch 1873 training_loss_per_epoch 0.5748732089996338\n","epoch 1874 training_loss_per_epoch 0.5748729705810547\n","epoch 1875 training_loss_per_epoch 0.5748727321624756\n","epoch 1876 training_loss_per_epoch 0.5748724341392517\n","epoch 1877 training_loss_per_epoch 0.5748721957206726\n","epoch 1878 training_loss_per_epoch 0.5748718976974487\n","epoch 1879 training_loss_per_epoch 0.5748716592788696\n","epoch 1880 training_loss_per_epoch 0.5748713612556458\n","epoch 1881 training_loss_per_epoch 0.5748711228370667\n","epoch 1882 training_loss_per_epoch 0.5748708248138428\n","epoch 1883 training_loss_per_epoch 0.5748705863952637\n","epoch 1884 training_loss_per_epoch 0.5748703479766846\n","epoch 1885 training_loss_per_epoch 0.5748701095581055\n","epoch 1886 training_loss_per_epoch 0.5748698115348816\n","epoch 1887 training_loss_per_epoch 0.5748695731163025\n","epoch 1888 training_loss_per_epoch 0.5748693346977234\n","epoch 1889 training_loss_per_epoch 0.5748690366744995\n","epoch 1890 training_loss_per_epoch 0.5748687982559204\n","epoch 1891 training_loss_per_epoch 0.5748685598373413\n","epoch 1892 training_loss_per_epoch 0.5748683214187622\n","epoch 1893 training_loss_per_epoch 0.5748680233955383\n","epoch 1894 training_loss_per_epoch 0.5748677849769592\n","epoch 1895 training_loss_per_epoch 0.5748676061630249\n","epoch 1896 training_loss_per_epoch 0.574867308139801\n","epoch 1897 training_loss_per_epoch 0.5748670101165771\n","epoch 1898 training_loss_per_epoch 0.5748668313026428\n","epoch 1899 training_loss_per_epoch 0.5748665928840637\n","epoch 1900 training_loss_per_epoch 0.5748663544654846\n","epoch 1901 training_loss_per_epoch 0.5748660564422607\n","epoch 1902 training_loss_per_epoch 0.5748658776283264\n","epoch 1903 training_loss_per_epoch 0.5748656392097473\n","epoch 1904 training_loss_per_epoch 0.5748654007911682\n","epoch 1905 training_loss_per_epoch 0.5748651027679443\n","epoch 1906 training_loss_per_epoch 0.57486492395401\n","epoch 1907 training_loss_per_epoch 0.5748646259307861\n","epoch 1908 training_loss_per_epoch 0.5748644471168518\n","epoch 1909 training_loss_per_epoch 0.5748642086982727\n","epoch 1910 training_loss_per_epoch 0.5748639106750488\n","epoch 1911 training_loss_per_epoch 0.5748637318611145\n","epoch 1912 training_loss_per_epoch 0.5748634934425354\n","epoch 1913 training_loss_per_epoch 0.5748632550239563\n","epoch 1914 training_loss_per_epoch 0.574863076210022\n","epoch 1915 training_loss_per_epoch 0.5748628377914429\n","epoch 1916 training_loss_per_epoch 0.5748625993728638\n","epoch 1917 training_loss_per_epoch 0.5748623609542847\n","epoch 1918 training_loss_per_epoch 0.5748621225357056\n","epoch 1919 training_loss_per_epoch 0.5748618841171265\n","epoch 1920 training_loss_per_epoch 0.5748615860939026\n","epoch 1921 training_loss_per_epoch 0.5748613476753235\n","epoch 1922 training_loss_per_epoch 0.5748611092567444\n","epoch 1923 training_loss_per_epoch 0.5748608708381653\n","epoch 1924 training_loss_per_epoch 0.5748606324195862\n","epoch 1925 training_loss_per_epoch 0.5748604536056519\n","epoch 1926 training_loss_per_epoch 0.574860155582428\n","epoch 1927 training_loss_per_epoch 0.5748599767684937\n","epoch 1928 training_loss_per_epoch 0.5748597383499146\n","epoch 1929 training_loss_per_epoch 0.5748594999313354\n","epoch 1930 training_loss_per_epoch 0.5748592615127563\n","epoch 1931 training_loss_per_epoch 0.574859082698822\n","epoch 1932 training_loss_per_epoch 0.5748588442802429\n","epoch 1933 training_loss_per_epoch 0.5748586058616638\n","epoch 1934 training_loss_per_epoch 0.5748583674430847\n","epoch 1935 training_loss_per_epoch 0.5748581290245056\n","epoch 1936 training_loss_per_epoch 0.5748578906059265\n","epoch 1937 training_loss_per_epoch 0.5748576521873474\n","epoch 1938 training_loss_per_epoch 0.5748574733734131\n","epoch 1939 training_loss_per_epoch 0.574857234954834\n","epoch 1940 training_loss_per_epoch 0.5748569965362549\n","epoch 1941 training_loss_per_epoch 0.5748567581176758\n","epoch 1942 training_loss_per_epoch 0.5748565793037415\n","epoch 1943 training_loss_per_epoch 0.5748563408851624\n","epoch 1944 training_loss_per_epoch 0.5748561024665833\n","epoch 1945 training_loss_per_epoch 0.5748558640480042\n","epoch 1946 training_loss_per_epoch 0.574855625629425\n","epoch 1947 training_loss_per_epoch 0.5748554468154907\n","epoch 1948 training_loss_per_epoch 0.5748552083969116\n","epoch 1949 training_loss_per_epoch 0.5748549699783325\n","epoch 1950 training_loss_per_epoch 0.5748547315597534\n","epoch 1951 training_loss_per_epoch 0.5748544931411743\n","epoch 1952 training_loss_per_epoch 0.5748542547225952\n","epoch 1953 training_loss_per_epoch 0.5748540759086609\n","epoch 1954 training_loss_per_epoch 0.5748538970947266\n","epoch 1955 training_loss_per_epoch 0.5748536586761475\n","epoch 1956 training_loss_per_epoch 0.5748534202575684\n","epoch 1957 training_loss_per_epoch 0.5748531818389893\n","epoch 1958 training_loss_per_epoch 0.5748529434204102\n","epoch 1959 training_loss_per_epoch 0.5748527646064758\n","epoch 1960 training_loss_per_epoch 0.574852466583252\n","epoch 1961 training_loss_per_epoch 0.5748522877693176\n","epoch 1962 training_loss_per_epoch 0.5748520493507385\n","epoch 1963 training_loss_per_epoch 0.5748518705368042\n","epoch 1964 training_loss_per_epoch 0.5748516321182251\n","epoch 1965 training_loss_per_epoch 0.574851393699646\n","epoch 1966 training_loss_per_epoch 0.5748511552810669\n","epoch 1967 training_loss_per_epoch 0.5748509764671326\n","epoch 1968 training_loss_per_epoch 0.5748506784439087\n","epoch 1969 training_loss_per_epoch 0.5748504996299744\n","epoch 1970 training_loss_per_epoch 0.57485032081604\n","epoch 1971 training_loss_per_epoch 0.5748500823974609\n","epoch 1972 training_loss_per_epoch 0.5748498439788818\n","epoch 1973 training_loss_per_epoch 0.5748496055603027\n","epoch 1974 training_loss_per_epoch 0.5748493671417236\n","epoch 1975 training_loss_per_epoch 0.5748491287231445\n","epoch 1976 training_loss_per_epoch 0.5748489499092102\n","epoch 1977 training_loss_per_epoch 0.5748487114906311\n","epoch 1978 training_loss_per_epoch 0.5748485326766968\n","epoch 1979 training_loss_per_epoch 0.5748482942581177\n","epoch 1980 training_loss_per_epoch 0.5748480558395386\n","epoch 1981 training_loss_per_epoch 0.5748478174209595\n","epoch 1982 training_loss_per_epoch 0.5748475790023804\n","epoch 1983 training_loss_per_epoch 0.5748473405838013\n","epoch 1984 training_loss_per_epoch 0.5748471021652222\n","epoch 1985 training_loss_per_epoch 0.5748469233512878\n","epoch 1986 training_loss_per_epoch 0.5748467445373535\n","epoch 1987 training_loss_per_epoch 0.5748464465141296\n","epoch 1988 training_loss_per_epoch 0.5748462677001953\n","epoch 1989 training_loss_per_epoch 0.5748460292816162\n","epoch 1990 training_loss_per_epoch 0.5748458504676819\n","epoch 1991 training_loss_per_epoch 0.5748456120491028\n","epoch 1992 training_loss_per_epoch 0.5748453736305237\n","epoch 1993 training_loss_per_epoch 0.5748451948165894\n","epoch 1994 training_loss_per_epoch 0.5748449563980103\n","epoch 1995 training_loss_per_epoch 0.5748447179794312\n","epoch 1996 training_loss_per_epoch 0.5748445391654968\n","epoch 1997 training_loss_per_epoch 0.5748443603515625\n","epoch 1998 training_loss_per_epoch 0.5748441219329834\n","epoch 1999 training_loss_per_epoch 0.5748439431190491\n","NDCG Ensemble 0.4850643449769524\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gJzVjvC8_N8p"},"source":["# References\n","\n","\n","\n","1.   Burges, Christopher JC. \"From ranknet to lambdarank to lambdamart: An overview.\" Learning 11.23-581 (2010): 81.\n","2. Kuzi, Saar, et al. \"Analysis of Adaptive Training for Learning to Rank in Information Retrieval.\" Proceedings of the 28th ACM International Conference on Information and Knowledge Management. 2019.\n","3.   https://towardsdatascience.com/how-to-code-a-simple-neural-network-in-pytorch-for-absolute-beginners-8f5209c50f\n","\n","\n"]}]}